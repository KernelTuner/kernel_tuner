<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial: From physics to tuned GPU kernels &mdash; Kernel Tuner 1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/nbsphinx-code-cells.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=359c27e9"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="contents.html" class="icon icon-home">
            Kernel Tuner
          </a>
              <div class="version">
                1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Kernel Tuner</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_multiplication.html">Matrix multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Kernel Tuner Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="backends.html">Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="cache_files.html">Cache files</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Correctness Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="hostcode.html">Tuning Host Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="structs.html">Using structs</a></li>
<li class="toctree-l1"><a class="reference internal" href="templates.html">Templated kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics and Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="observers.html">Observers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user-api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="vocabulary.html">Parameter Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="design.html">Design documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribution guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="contents.html">Kernel Tuner</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="contents.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Tutorial: From physics to tuned GPU kernels</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/KernelTuner/kernel_tuner/blob/master/doc/source/diffusion_use_optparam.ipynb" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Tutorial:-From-physics-to-tuned-GPU-kernels">
<h1>Tutorial: From physics to tuned GPU kernels<a class="headerlink" href="#Tutorial:-From-physics-to-tuned-GPU-kernels" title="Permalink to this heading">¶</a></h1>
<p>This tutorial is designed to show you the whole process starting from modeling a physical process to a Python implementation to creating optimized and auto-tuned GPU application using Kernel Tuner.</p>
<p>In this tutorial, we will use <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion">diffusion</a> as an example application.</p>
<p>We start with modeling the physical process of diffusion, for which we create a simple numerical implementation in Python. Then we create a CUDA kernel that performs the same computation, but on the GPU. Once we have a CUDA kernel, we start using the Kernel Tuner for auto-tuning our GPU application. And finally, we’ll introduce a few code optimizations to our CUDA kernel that will improve performance, but also add more parameters to tune on using the Kernel Tuner.</p>
<div class="admonition note">
<p><strong>Note:</strong> If you are reading this tutorial on the Kernel Tuner’s documentation pages, note that you can actually run this tutorial as a Jupyter Notebook. Just clone the Kernel Tuner’s <a class="reference external" href="http://github.com/benvanwerkhoven/kernel_tuner">GitHub repository</a>. Install the Kernel Tuner and Jupyter Notebooks and you’re ready to go! You can start the tutorial by typing “jupyter notebook” in the “kernel_tuner/doc/source” directory.</p>
</div>
<section id="Diffusion">
<h2>Diffusion<a class="headerlink" href="#Diffusion" title="Permalink to this heading">¶</a></h2>
<p>Put simply, diffusion is the redistribution of something from a region of high concentration to a region of low concentration without bulk motion. The concept of diffusion is widely used in many fields, including physics, chemistry, biology, and many more.</p>
<p>Suppose that we take a metal sheet, in which the temperature is exactly equal to one degree everywhere in the sheet. Now if we were to heat a number of points on the sheet to a very high temperature, say a thousand degrees, in an instant by some method. We could see the heat diffuse from these hotspots to the cooler areas. We are assuming that the metal does not melt. In addition, we will ignore any heat loss from radiation or other causes in this example.</p>
<p>We can use the <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion_equation">diffusion equation</a> to model how the heat diffuses through our metal sheet:</p>
<p><span class="math">\begin{equation*}
\frac{\partial u}{\partial t}= D \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right)
\end{equation*}</span></p>
<p>Where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> represent the spatial descretization of our 2D domain, <span class="math notranslate nohighlight">\(u\)</span> is the quantity that is being diffused, <span class="math notranslate nohighlight">\(t\)</span> is the descretization in time, and the constant <span class="math notranslate nohighlight">\(D\)</span> determines how fast the diffusion takes place.</p>
<p>In this example, we will assume a very simple descretization of our problem. We assume that our 2D domain has <span class="math notranslate nohighlight">\(nx\)</span> equi-distant grid points in the x-direction and <span class="math notranslate nohighlight">\(ny\)</span> equi-distant grid points in the y-direction. Be sure to execute every cell as you read through this document, by selecting it and pressing <strong>shift+enter</strong>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nx</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">ny</span> <span class="o">=</span> <span class="mi">1024</span>
</pre></div>
</div>
</div>
<p>This results in a constant distance of <span class="math notranslate nohighlight">\(\delta x\)</span> between all grid points in the <span class="math notranslate nohighlight">\(x\)</span> dimension. Using central differences, we can numerically approximate the derivative for a given point <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<p><span class="math">\begin{equation*}
\left. \frac{\partial^2 u}{\partial x^2} \right|_{x_{i}} \approx \frac{u_{x_{i+1}}-2u_{{x_i}}+u_{x_{i-1}}}{(\delta x)^2}
\end{equation*}</span></p>
<p>We do the same for the partial derivative in <span class="math notranslate nohighlight">\(y\)</span>:</p>
<p><span class="math">\begin{equation*}
\left. \frac{\partial^2 u}{\partial y^2} \right|_{y_{i}} \approx \frac{u_{y_{i+1}}-2u_{y_{i}}+u_{y_{i-1}}}{(\delta y)^2}
\end{equation*}</span></p>
<p>If we combine the above equations, we can obtain a numerical estimation for the temperature field of our metal sheet in the next time step, using <span class="math notranslate nohighlight">\(\delta t\)</span> as the time between time steps. But before we do, we also simplify the expression a little bit, because we’ll assume that <span class="math notranslate nohighlight">\(\delta x\)</span> and <span class="math notranslate nohighlight">\(\delta y\)</span> are always equal to 1.</p>
<p><span class="math">\begin{equation*}
u'_{x,y} = u_{x,y} + \delta t \times \left( \left( u_{x_{i+1},y}-2u_{{x_i},y}+u_{x_{i-1},y} \right) + \left( u_{x,y_{i+1}}-2u_{x,y_{i}}+u_{x,y_{i-1}} \right) \right)
\end{equation*}</span></p>
<p>In this formula <span class="math notranslate nohighlight">\(u'_{x,y}\)</span> refers to the temperature field at the time <span class="math notranslate nohighlight">\(t + \delta t\)</span>. As a final step, we further simplify this equation to:</p>
<p><span class="math">\begin{equation*}
u'_{x,y} = u_{x,y} + \delta t \times \left( u_{x,y_{i+1}}+u_{x_{i+1},y}-4u_{{x_i},y}+u_{x_{i-1},y}+u_{x,y_{i-1}} \right)
\end{equation*}</span></p>
</section>
<section id="Python-implementation">
<h2>Python implementation<a class="headerlink" href="#Python-implementation" title="Permalink to this heading">¶</a></h2>
<p>We can create a Python function that implements the numerical approximation defined in the above equation. For simplicity we’ll use the assumption of a free boundary condition.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">diffuse</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.225</span><span class="p">):</span>
    <span class="n">field</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">nx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="n">ny</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">nx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="n">ny</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">field</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">nx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">:</span><span class="n">ny</span><span class="p">]</span><span class="o">+</span><span class="n">field</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="n">nx</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="n">ny</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">field</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">nx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="n">ny</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span>
        <span class="n">field</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nx</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="n">ny</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">field</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">nx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">ny</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="p">)</span>
    <span class="k">return</span> <span class="n">field</span>
</pre></div>
</div>
</div>
<p>To give our Python function a test run, we will now do some imports and generate the input data for the initial conditions of our metal sheet with a few very hot points. We’ll also make two plots, one after a thousand time steps, and a second plot after another two thousand time steps. Do note that the plots are using different ranges for the colors. Also, executing the following cell may take a little while.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1">#setup initial conditions</span>
<span class="k">def</span> <span class="nf">get_initial_conditions</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">):</span>
    <span class="n">field</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">field</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">nx</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">ny</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1e3</span>
    <span class="k">return</span> <span class="n">field</span>
<span class="n">field</span> <span class="o">=</span> <span class="n">get_initial_conditions</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can now use this initial condition to solve the diffusion problem and plot the results.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1">#run the diffuse function a 1000 times and another 2000 times and make plots</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">cpu</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">cpu</span> <span class="o">=</span> <span class="n">diffuse</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">cpu</span> <span class="o">=</span> <span class="n">diffuse</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x7f888f8cd7b8&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_use_optparam_9_1.png" src="_images/diffusion_use_optparam_9_1.png" />
</div>
</div>
<p>Now let’s take a quick look at the execution time of our diffuse function. Before we do, we also copy the current state of the metal sheet to be able to restart the computation from this state.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#run another 1000 steps of the diffuse function and measure the time</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">cpu</span><span class="o">=</span><span class="n">numpy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">cpu</span> <span class="o">=</span> <span class="n">diffuse</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1000 steps of diffuse on a </span><span class="si">%d</span><span class="s2"> x </span><span class="si">%d</span><span class="s2"> grid took&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">),</span> <span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="o">*</span><span class="mf">1000.0</span><span class="p">,</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
1000 steps of diffuse on a 1024 x 1024 grid took 4152.086019515991 ms
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x7f8865b51f28&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_use_optparam_11_2.png" src="_images/diffusion_use_optparam_11_2.png" />
</div>
</div>
</section>
<section id="Computing-on-the-GPU">
<h2>Computing on the GPU<a class="headerlink" href="#Computing-on-the-GPU" title="Permalink to this heading">¶</a></h2>
<p>The next step in this tutorial is to implement a GPU kernel that will allow us to run our problem on the GPU. We store the kernel code in a Python string, because we can directly compile and run the kernel from Python. In this tutorial, we’ll use the CUDA programming model to implement our kernels.</p>
<blockquote>
<div><p>If you prefer OpenCL over CUDA, don’t worry. Everything in this tutorial applies as much to OpenCL as it does to CUDA. But we will use CUDA for our examples, and CUDA terminology in the text.</p>
</div></blockquote>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_kernel_string</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    #define nx </span><span class="si">%d</span>
<span class="s2">    #define ny </span><span class="si">%d</span>
<span class="s2">    #define dt 0.225f</span>
<span class="s2">    __global__ void diffuse_kernel(float *u_new, float *u) {</span>
<span class="s2">        int x = blockIdx.x * block_size_x + threadIdx.x;</span>
<span class="s2">        int y = blockIdx.y * block_size_y + threadIdx.y;</span>

<span class="s2">        if (x&gt;0 &amp;&amp; x&lt;nx-1 &amp;&amp; y&gt;0 &amp;&amp; y&lt;ny-1) {</span>
<span class="s2">            u_new[y*nx+x] = u[y*nx+x] + dt * (</span>
<span class="s2">                u[(y+1)*nx+x]+u[y*nx+x+1]-4.0f*u[y*nx+x]+u[y*nx+x-1]+u[(y-1)*nx+x]);</span>
<span class="s2">        }</span>
<span class="s2">    }</span>
<span class="s2">    &quot;&quot;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
<span class="n">kernel_string</span> <span class="o">=</span> <span class="n">get_kernel_string</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The above CUDA kernel parallelizes the work such that every grid point will be processed by a different CUDA thread. Therefore, the kernel is executed by a 2D grid of threads, which are grouped together into 2D thread blocks. The specific thread block dimensions we choose are not important for the result of the computation in this kernel. But as we will see will later, they will have an impact on performance.</p>
<p>In this kernel we are using two, currently undefined, compile-time constants for <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code>, because we will auto tune these parameters later. It is often needed for performance to fix the thread block dimensions at compile time, because the compiler can unroll loops that iterate using the block size, or because you need to allocate shared memory using the thread block dimensions.</p>
<p>The next bit of Python code initializes PyCuda, and makes preparations so that we can call the CUDA kernel to do the computation on the GPU as we did earlier in Python.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pycuda</span> <span class="kn">import</span> <span class="n">driver</span><span class="p">,</span> <span class="n">compiler</span><span class="p">,</span> <span class="n">gpuarray</span><span class="p">,</span> <span class="n">tools</span>
<span class="kn">import</span> <span class="nn">pycuda.autoinit</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="c1">#allocate GPU memory</span>
<span class="n">u_old</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>
<span class="n">u_new</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>

<span class="c1">#setup thread block dimensions and compile the kernel</span>
<span class="n">threads</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">nx</span><span class="o">/</span><span class="mi">16</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">ny</span><span class="o">/</span><span class="mi">16</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">block_size_string</span> <span class="o">=</span> <span class="s2">&quot;#define block_size_x 16</span><span class="se">\n</span><span class="s2">#define block_size_y 16</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">SourceModule</span><span class="p">(</span><span class="n">block_size_string</span><span class="o">+</span><span class="n">kernel_string</span><span class="p">)</span>
<span class="n">diffuse_kernel</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s2">&quot;diffuse_kernel&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The above code is a bit of boilerplate we need to compile a kernel using PyCuda. We’ve also, for the moment, fixed the thread block dimensions at 16 by 16. These dimensions serve as our initial guess for what a good performing pair of thread block dimensions could look like.</p>
<p>Now that we’ve setup everything, let’s see how long the computation would take using the GPU.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#call the GPU kernel a 1000 times and measure performance</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">diffuse_kernel</span><span class="p">(</span><span class="n">u_new</span><span class="p">,</span> <span class="n">u_old</span><span class="p">,</span> <span class="n">block</span><span class="o">=</span><span class="n">threads</span><span class="p">,</span> <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>
    <span class="n">diffuse_kernel</span><span class="p">(</span><span class="n">u_old</span><span class="p">,</span> <span class="n">u_new</span><span class="p">,</span> <span class="n">block</span><span class="o">=</span><span class="n">threads</span><span class="p">,</span> <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>
<span class="n">driver</span><span class="o">.</span><span class="n">Context</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1000 steps of diffuse ona </span><span class="si">%d</span><span class="s2"> x </span><span class="si">%d</span><span class="s2"> grid took&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">),</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="s2">&quot;ms.&quot;</span><span class="p">)</span>

<span class="c1">#copy the result from the GPU to Python for plotting</span>
<span class="n">gpu_result</span> <span class="o">=</span> <span class="n">u_old</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">gpu_result</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;GPU Result&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Python Result&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
1000 steps of diffuse ona 1024 x 1024 grid took 33.46109390258789 ms.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.text.Text at 0x7f8858b873c8&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_use_optparam_17_2.png" src="_images/diffusion_use_optparam_17_2.png" />
</div>
</div>
<p>That should already be a lot faster than our previous Python implementation, but we can do much better if we optimize our GPU kernel. And that is exactly what the rest of this tutorial is about!</p>
<p>Also, if you think the Python boilerplate code to call a GPU kernel was a bit messy, we’ve got good news for you! From now on, we’ll only use the Kernel Tuner to compile and benchmark GPU kernels, which we can do with much cleaner Python code.</p>
</section>
<section id="Auto-Tuning-with-the-Kernel-Tuner">
<h2>Auto-Tuning with the Kernel Tuner<a class="headerlink" href="#Auto-Tuning-with-the-Kernel-Tuner" title="Permalink to this heading">¶</a></h2>
<p>Remember that previously we’ve set the thread block dimensions to 16 by 16. But how do we actually know if that is the best performing setting? That is where auto-tuning comes into play. Basically, it is very difficult to provide an answer through performance modeling and as such, we’d rather use the Kernel Tuner to compile and benchmark all possible kernel configurations.</p>
<p>But before we continue, we’ll increase the problem size, because the GPU is very likely underutilized.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nx</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">ny</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">field</span> <span class="o">=</span> <span class="n">get_initial_conditions</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
<span class="n">kernel_string</span> <span class="o">=</span> <span class="n">get_kernel_string</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The above code block has generated new initial conditions and a new string that contains our CUDA kernel using our new domain size.</p>
<p>To call the Kernel Tuner, we have to specify the tunable parameters, in our case <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code>. For this purpose, we’ll create an ordered dictionary to store the tunable parameters. The keys will be the name of the tunable parameter, and the corresponding value is the list of possible values for the parameter. For the purpose of this tutorial, we’ll use a small number of commonly used values for the thread block dimensions, but feel free to try more!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="n">tune_params</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;block_size_x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
<span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;block_size_y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>We also have to tell the Kernel Tuner about the argument list of our CUDA kernel. Because the Kernel Tuner will be calling the CUDA kernel and measure its execution time. For this purpose we create a list in Python, that corresponds with the argument list of the <code class="docutils literal notranslate"><span class="pre">diffuse_kernel</span></code> CUDA function. This list will only be used as input to the kernel during tuning. The objects in the list should be Numpy arrays or scalars.</p>
<p>Because you can specify the arguments as Numpy arrays, the Kernel Tuner will take care of allocating GPU memory and copying the data to the GPU.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">field</span><span class="p">,</span> <span class="n">field</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>We’re almost ready to call the Kernel Tuner, we just need to set how large the problem is we are currently working on by setting a <code class="docutils literal notranslate"><span class="pre">problem_size</span></code>. The Kernel Tuner knows about thread block dimensions, which it expects to be called <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code>, <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code>, and/or <code class="docutils literal notranslate"><span class="pre">block_size_z</span></code>. From these and the <code class="docutils literal notranslate"><span class="pre">problem_size</span></code>, the Kernel Tuner will compute the appropiate grid dimensions on the fly.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">problem_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>And that’s everything the Kernel Tuner needs to know to be able to start tuning our kernel. Let’s give it a try by executing the next code block!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">kernel_tuner</span> <span class="kn">import</span> <span class="n">tune_kernel</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tune_kernel</span><span class="p">(</span><span class="s2">&quot;diffuse_kernel&quot;</span><span class="p">,</span> <span class="n">kernel_string</span><span class="p">,</span> <span class="n">problem_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">tune_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using: GeForce GTX 1080 Ti
block_size_x=16, block_size_y=2, time=0.916985595226
block_size_x=16, block_size_y=4, time=0.489004802704
block_size_x=16, block_size_y=8, time=0.500524806976
block_size_x=16, block_size_y=16, time=0.513356792927
block_size_x=16, block_size_y=32, time=0.545715200901
block_size_x=32, block_size_y=2, time=0.486515200138
block_size_x=32, block_size_y=4, time=0.449055999517
block_size_x=32, block_size_y=8, time=0.44974719882
block_size_x=32, block_size_y=16, time=0.457427197695
block_size_x=32, block_size_y=32, time=0.492915201187
block_size_x=48, block_size_y=2, time=0.464863997698
block_size_x=48, block_size_y=4, time=0.466118401289
block_size_x=48, block_size_y=8, time=0.475264000893
block_size_x=48, block_size_y=16, time=0.513632011414
block_size_x=64, block_size_y=2, time=0.458412796259
block_size_x=64, block_size_y=4, time=0.457715201378
block_size_x=64, block_size_y=8, time=0.461017608643
block_size_x=64, block_size_y=16, time=0.475987195969
block_size_x=128, block_size_y=2, time=0.460032004118
block_size_x=128, block_size_y=4, time=0.457779198885
block_size_x=128, block_size_y=8, time=0.462649595737
best performing configuration: block_size_x=32, block_size_y=4, time=0.449055999517
</pre></div></div>
</div>
<p>Note that the Kernel Tuner prints a lot of useful information. To ensure you’ll be able to tell what was measured in this run the Kernel Tuner always prints the GPU or OpenCL Device name that is being used, as well as the name of the kernel. After that every line contains the combination of parameters and the time that was measured during benchmarking. The time that is being printed is in milliseconds and is obtained by averaging the execution time of 7 runs of the kernel. Finally, as a matter
of convenience, the Kernel Tuner also prints the best performing combination of tunable parameters. However, later on in this tutorial we’ll explain how to analyze and store the tuning results using Python.</p>
<p>Looking at the results printed above, the difference in performance between the different kernel configurations may seem very little. However, on our hardware, the performance of this kernel already varies in the order of 10%. Which of course can build up to large differences in the execution time if the kernel is to be executed thousands of times. We can also see that the performance of the best configuration in this set is 5% better than our initially guessed thread block dimensions of 16 by
16.</p>
<p>In addtion, you may notice that not all possible combinations of values for <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code> are among the results. For example, 128x32 is not among the results. This is because some configuration require more threads per thread block than allowed on our GPU. The Kernel Tuner checks the limitations of your GPU at runtime and automatically skips over configurations that use too many threads per block. It will also do this for kernels that cannot be compiled because they use
too much shared memory. And likewise for kernels that use too many registers to be launched at runtime. If you’d like to know about which configurations were skipped automatically you can pass the optional parameter <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> to <code class="docutils literal notranslate"><span class="pre">tune_kernel</span></code>.</p>
<p>However, knowing the best performing combination of tunable parameters becomes even more important when we start to further optimize our CUDA kernel. In the next section, we’ll add a simple code optimization and show how this affects performance.</p>
</section>
<section id="Using-shared-memory">
<h2>Using shared memory<a class="headerlink" href="#Using-shared-memory" title="Permalink to this heading">¶</a></h2>
<p>Shared memory, is a special type of the memory available in CUDA. Shared memory can be used by threads within the same thread block to exchange and share values. It is in fact, one of the very few ways for threads to communicate on the GPU.</p>
<p>The idea is that we’ll try improve the performance of our kernel by using shared memory as a software controlled cache. There are already caches on the GPU, but most GPUs only cache accesses to global memory in L2. Shared memory is closer to the multiprocessors where the thread blocks are executed, comparable to an L1 cache.</p>
<p>However, because there are also hardware caches, the performance improvement from this step is expected to not be that great. The more fine-grained control that we get by using a software managed cache, rather than a hardware implemented cache, comes at the cost of some instruction overhead. In fact, performance is quite likely to degrade a little. However, this intermediate step is necessary for the next optimization step we have in mind.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel_string_shared</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#define nx </span><span class="si">%d</span>
<span class="s2">#define ny </span><span class="si">%d</span>
<span class="s2">#define dt 0.225f</span>
<span class="s2">__global__ void diffuse_kernel(float *u_new, float *u) {</span>

<span class="s2">    int tx = threadIdx.x;</span>
<span class="s2">    int ty = threadIdx.y;</span>
<span class="s2">    int bx = blockIdx.x * block_size_x;</span>
<span class="s2">    int by = blockIdx.y * block_size_y;</span>

<span class="s2">    __shared__ float sh_u[block_size_y+2][block_size_x+2];</span>

<span class="s2">    #pragma unroll</span>
<span class="s2">    for (int i = ty; i&lt;block_size_y+2; i+=block_size_y) {</span>
<span class="s2">        #pragma unroll</span>
<span class="s2">        for (int j = tx; j&lt;block_size_x+2; j+=block_size_x) {</span>
<span class="s2">            int y = by+i-1;</span>
<span class="s2">            int x = bx+j-1;</span>
<span class="s2">            if (x&gt;=0 &amp;&amp; x&lt;nx &amp;&amp; y&gt;=0 &amp;&amp; y&lt;ny) {</span>
<span class="s2">                sh_u[i][j] = u[y*nx+x];</span>
<span class="s2">            }</span>
<span class="s2">        }</span>
<span class="s2">    }</span>
<span class="s2">    __syncthreads();</span>

<span class="s2">    int x = bx+tx;</span>
<span class="s2">    int y = by+ty;</span>
<span class="s2">    if (x&gt;0 &amp;&amp; x&lt;nx-1 &amp;&amp; y&gt;0 &amp;&amp; y&lt;ny-1) {</span>
<span class="s2">        int i = ty+1;</span>
<span class="s2">        int j = tx+1;</span>
<span class="s2">        u_new[y*nx+x] = sh_u[i][j] + dt * (</span>
<span class="s2">            sh_u[i+1][j] + sh_u[i][j+1] -4.0f * sh_u[i][j] +</span>
<span class="s2">            sh_u[i][j-1] + sh_u[i-1][j] );</span>
<span class="s2">    }</span>

<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can now tune this new kernel using the kernel tuner</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">tune_kernel</span><span class="p">(</span><span class="s2">&quot;diffuse_kernel&quot;</span><span class="p">,</span> <span class="n">kernel_string_shared</span><span class="p">,</span> <span class="n">problem_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">tune_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using: GeForce GTX 1080 Ti
block_size_x=16, block_size_y=2, time=1.22673916817
block_size_x=16, block_size_y=4, time=0.826361596584
block_size_x=16, block_size_y=8, time=0.793516802788
block_size_x=16, block_size_y=16, time=0.782112002373
block_size_x=16, block_size_y=32, time=0.776639997959
block_size_x=32, block_size_y=2, time=0.795135998726
block_size_x=32, block_size_y=4, time=0.722777605057
block_size_x=32, block_size_y=8, time=0.762777590752
block_size_x=32, block_size_y=16, time=0.75422719717
block_size_x=32, block_size_y=32, time=0.804876792431
block_size_x=48, block_size_y=2, time=0.778656005859
block_size_x=48, block_size_y=4, time=0.769734406471
block_size_x=48, block_size_y=8, time=0.782495999336
block_size_x=48, block_size_y=16, time=0.932281601429
block_size_x=64, block_size_y=2, time=0.734028804302
block_size_x=64, block_size_y=4, time=0.721625590324
block_size_x=64, block_size_y=8, time=0.736511993408
block_size_x=64, block_size_y=16, time=0.800019192696
block_size_x=128, block_size_y=2, time=0.724966406822
block_size_x=128, block_size_y=4, time=0.722969603539
block_size_x=128, block_size_y=8, time=0.759430396557
best performing configuration: block_size_x=64, block_size_y=4, time=0.721625590324
</pre></div></div>
</div>
</section>
<section id="Tiling-GPU-Code">
<h2>Tiling GPU Code<a class="headerlink" href="#Tiling-GPU-Code" title="Permalink to this heading">¶</a></h2>
<p>One very useful code optimization is called tiling, sometimes also called thread-block-merge. You can look at it in this way, currently we have many thread blocks that together work on the entire domain. If we were to use only half of the number of thread blocks, every thread block would need to double the amount of work it performs to cover the entire domain. However, the threads may be able to reuse part of the data and computation that is required to process a single output element for every
element beyond the first.</p>
<p>This is a code optimization because effectively we are reducing the total number of instructions executed by all threads in all thread blocks. So in a way, were are condensing the total instruction stream while keeping the all the really necessary compute instructions. More importantly, we are increasing data reuse, where previously these values would have been reused from the cache or in the worst-case from GPU memory.</p>
<p>We can apply tiling in both the x and y-dimensions. This also introduces two new tunable parameters, namely the tiling factor in x and y, which we will call <code class="docutils literal notranslate"><span class="pre">tile_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">tile_size_y</span></code>. This is what the new kernel looks like:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel_string_tiled</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#define nx </span><span class="si">%d</span>
<span class="s2">#define ny </span><span class="si">%d</span>
<span class="s2">#define dt 0.225f</span>
<span class="s2">__global__ void diffuse_kernel(float *u_new, float *u) {</span>

<span class="s2">    int tx = threadIdx.x;</span>
<span class="s2">    int ty = threadIdx.y;</span>
<span class="s2">    int bx = blockIdx.x * block_size_x * tile_size_x;</span>
<span class="s2">    int by = blockIdx.y * block_size_y * tile_size_y;</span>

<span class="s2">    __shared__ float sh_u[block_size_y*tile_size_y+2][block_size_x*tile_size_x+2];</span>

<span class="s2">    #pragma unroll</span>
<span class="s2">    for (int i = ty; i&lt;block_size_y*tile_size_y+2; i+=block_size_y) {</span>
<span class="s2">        #pragma unroll</span>
<span class="s2">        for (int j = tx; j&lt;block_size_x*tile_size_x+2; j+=block_size_x) {</span>
<span class="s2">            int y = by+i-1;</span>
<span class="s2">            int x = bx+j-1;</span>
<span class="s2">            if (x&gt;=0 &amp;&amp; x&lt;nx &amp;&amp; y&gt;=0 &amp;&amp; y&lt;ny) {</span>
<span class="s2">                sh_u[i][j] = u[y*nx+x];</span>
<span class="s2">            }</span>
<span class="s2">        }</span>
<span class="s2">    }</span>
<span class="s2">    __syncthreads();</span>

<span class="s2">    #pragma unroll</span>
<span class="s2">    for (int tj=0; tj&lt;tile_size_y; tj++) {</span>
<span class="s2">        int i = ty+tj*block_size_y+1;</span>
<span class="s2">        int y = by + ty + tj*block_size_y;</span>
<span class="s2">        #pragma unroll</span>
<span class="s2">        for (int ti=0; ti&lt;tile_size_x; ti++) {</span>
<span class="s2">            int j = tx+ti*block_size_x+1;</span>
<span class="s2">            int x = bx + tx + ti*block_size_x;</span>
<span class="s2">            if (x&gt;0 &amp;&amp; x&lt;nx-1 &amp;&amp; y&gt;0 &amp;&amp; y&lt;ny-1) {</span>
<span class="s2">                u_new[y*nx+x] = sh_u[i][j] + dt * (</span>
<span class="s2">                    sh_u[i+1][j] + sh_u[i][j+1] -4.0f * sh_u[i][j] +</span>
<span class="s2">                    sh_u[i][j-1] + sh_u[i-1][j] );</span>
<span class="s2">            }</span>
<span class="s2">        }</span>

<span class="s2">    }</span>

<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can tune our tiled kernel by adding the two new tunable parameters to our dictionary <code class="docutils literal notranslate"><span class="pre">tune_params</span></code>.</p>
<p>We also need to somehow tell the Kernel Tuner to use fewer thread blocks to launch kernels with <code class="docutils literal notranslate"><span class="pre">tile_size_x</span></code> or <code class="docutils literal notranslate"><span class="pre">tile_size_y</span></code> larger than one. For this purpose the Kernel Tuner’s <code class="docutils literal notranslate"><span class="pre">tune_kernel</span></code> function supports two optional arguments, called grid_div_x and grid_div_y. These are the grid divisor lists, which are lists of strings containing all the tunable parameters that divide a certain grid dimension. So far, we have been using the default settings for these, in which case the Kernel
Tuner only uses the block_size_x and block_size_y tunable parameters to divide the problem_size.</p>
<p>Note that the Kernel Tuner will replace the values of the tunable parameters inside the strings and use the product of the parameters in the grid divisor list to compute the grid dimension rounded up. You can even use arithmetic operations, inside these strings as they will be evaluated. As such, we could have used <code class="docutils literal notranslate"><span class="pre">[&quot;block_size_x*tile_size_x&quot;]</span></code> to get the same result.</p>
<p>We are now ready to call the Kernel Tuner again and tune our tiled kernel. Let’s execute the following code block, note that it may take a while as the number of kernel configurations that the Kernel Tuner will try has just been increased with a factor of 9!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;tile_size_x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>            <span class="c1">#add tile_size_x to the tune_params</span>
<span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;tile_size_y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>            <span class="c1">#add tile_size_y to the tune_params</span>
<span class="n">grid_div_x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;block_size_x&quot;</span><span class="p">,</span> <span class="s2">&quot;tile_size_x&quot;</span><span class="p">]</span>    <span class="c1">#tile_size_x impacts grid dimensions</span>
<span class="n">grid_div_y</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;block_size_y&quot;</span><span class="p">,</span> <span class="s2">&quot;tile_size_y&quot;</span><span class="p">]</span>    <span class="c1">#tile_size_y impacts grid dimensions</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">tune_kernel</span><span class="p">(</span><span class="s2">&quot;diffuse_kernel&quot;</span><span class="p">,</span> <span class="n">kernel_string_tiled</span><span class="p">,</span> <span class="n">problem_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span>
                     <span class="n">tune_params</span><span class="p">,</span> <span class="n">grid_div_x</span><span class="o">=</span><span class="n">grid_div_x</span><span class="p">,</span> <span class="n">grid_div_y</span><span class="o">=</span><span class="n">grid_div_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using: GeForce GTX 1080 Ti
block_size_x=16, block_size_y=2, tile_size_x=1, tile_size_y=1, time=1.22200961113
block_size_x=16, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.91601279974
block_size_x=16, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.752838408947
block_size_x=16, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.873651194572
block_size_x=16, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.69833599329
block_size_x=16, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.586931192875
block_size_x=16, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.516473591328
block_size_x=16, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.411392003298
block_size_x=16, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.384262400866
block_size_x=16, block_size_y=4, tile_size_x=1, tile_size_y=1, time=0.82159358263
block_size_x=16, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.632607996464
block_size_x=16, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.506457602978
block_size_x=16, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.618758392334
block_size_x=16, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.500288009644
block_size_x=16, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.429862397909
block_size_x=16, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.44995200038
block_size_x=16, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.366150397062
block_size_x=16, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.342201602459
block_size_x=16, block_size_y=8, tile_size_x=1, tile_size_y=1, time=0.793542397022
block_size_x=16, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.58026239872
block_size_x=16, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.494163197279
block_size_x=16, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.546316814423
block_size_x=16, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.467059195042
block_size_x=16, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.404249596596
block_size_x=16, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.440895992517
block_size_x=16, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.341376006603
block_size_x=16, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.339692795277
block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=1, time=0.783923208714
block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.597920000553
block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.50277120471
block_size_x=16, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.615475213528
block_size_x=16, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.470937597752
block_size_x=16, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.418393599987
block_size_x=16, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.443519997597
block_size_x=16, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.343961596489
block_size_x=16, block_size_y=16, tile_size_x=4, tile_size_y=4, time=0.342540800571
block_size_x=16, block_size_y=32, tile_size_x=1, tile_size_y=1, time=0.780352008343
block_size_x=16, block_size_y=32, tile_size_x=1, tile_size_y=2, time=0.611705589294
block_size_x=16, block_size_y=32, tile_size_x=1, tile_size_y=4, time=0.515667212009
block_size_x=16, block_size_y=32, tile_size_x=2, tile_size_y=1, time=0.622534394264
block_size_x=16, block_size_y=32, tile_size_x=2, tile_size_y=2, time=0.502195191383
block_size_x=16, block_size_y=32, tile_size_x=2, tile_size_y=4, time=0.437388807535
block_size_x=16, block_size_y=32, tile_size_x=4, tile_size_y=1, time=0.45568639636
block_size_x=16, block_size_y=32, tile_size_x=4, tile_size_y=2, time=0.359289598465
block_size_x=16, block_size_y=32, tile_size_x=4, tile_size_y=4, time=0.426995199919
block_size_x=32, block_size_y=2, tile_size_x=1, tile_size_y=1, time=0.788947200775
block_size_x=32, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.616556799412
block_size_x=32, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.496121603251
block_size_x=32, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.629164803028
block_size_x=32, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.474841600657
block_size_x=32, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.407667201757
block_size_x=32, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.47406719923
block_size_x=32, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.371507203579
block_size_x=32, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.352531200647
block_size_x=32, block_size_y=4, tile_size_x=1, tile_size_y=1, time=0.72023679018
block_size_x=32, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.574816000462
block_size_x=32, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.481817597151
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.580928003788
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.455724793673
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.394975996017
block_size_x=32, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.464659202099
block_size_x=32, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.357107198238
block_size_x=32, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.324083191156
block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=1, time=0.759910392761
block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.569177603722
block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.481279999018
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.528115200996
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.441734397411
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.393126398325
block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.455404800177
block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.350457596779
block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.322547197342
block_size_x=32, block_size_y=16, tile_size_x=1, tile_size_y=1, time=0.754201591015
block_size_x=32, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.579827189445
block_size_x=32, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.491852802038
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.582751989365
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.451283198595
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.391807991266
block_size_x=32, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.456275194883
block_size_x=32, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.356716805696
block_size_x=32, block_size_y=16, tile_size_x=4, tile_size_y=4, time=0.362937599421
block_size_x=32, block_size_y=32, tile_size_x=1, tile_size_y=1, time=0.809894394875
block_size_x=32, block_size_y=32, tile_size_x=1, tile_size_y=2, time=0.60433280468
block_size_x=32, block_size_y=32, tile_size_x=1, tile_size_y=4, time=0.507142400742
block_size_x=32, block_size_y=32, tile_size_x=2, tile_size_y=1, time=0.655827200413
block_size_x=32, block_size_y=32, tile_size_x=2, tile_size_y=2, time=0.474092799425
block_size_x=32, block_size_y=32, tile_size_x=2, tile_size_y=4, time=0.408166396618
block_size_x=32, block_size_y=32, tile_size_x=4, tile_size_y=1, time=0.480531209707
block_size_x=32, block_size_y=32, tile_size_x=4, tile_size_y=2, time=0.346707201004
block_size_x=48, block_size_y=2, tile_size_x=1, tile_size_y=1, time=0.780134403706
block_size_x=48, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.601049602032
block_size_x=48, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.493900799751
block_size_x=48, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.620384001732
block_size_x=48, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.494553589821
block_size_x=48, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.425414395332
block_size_x=48, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.467033600807
block_size_x=48, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.375468802452
block_size_x=48, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.346079999208
block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=1, time=0.771052801609
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.593977594376
block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.49723520875
block_size_x=48, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.583270406723
block_size_x=48, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.478079998493
block_size_x=48, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.416320002079
block_size_x=48, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.443942397833
block_size_x=48, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.359744000435
block_size_x=48, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.343545603752
block_size_x=48, block_size_y=8, tile_size_x=1, tile_size_y=1, time=0.780960011482
block_size_x=48, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.598758399487
block_size_x=48, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.498617601395
block_size_x=48, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.57678719759
block_size_x=48, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.46561280489
block_size_x=48, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.41324160099
block_size_x=48, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.431225597858
block_size_x=48, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.351263999939
block_size_x=48, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.34440960288
block_size_x=48, block_size_y=16, tile_size_x=1, tile_size_y=1, time=0.933260798454
block_size_x=48, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.715257608891
block_size_x=48, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.586604809761
block_size_x=48, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.711615991592
block_size_x=48, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.558771193027
block_size_x=48, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.466284793615
block_size_x=48, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.44043520093
block_size_x=48, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.361823999882
block_size_x=64, block_size_y=2, tile_size_x=1, tile_size_y=1, time=0.731839990616
block_size_x=64, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.57044479847
block_size_x=64, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.470220798254
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.608800005913
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.472665601969
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.416352003813
block_size_x=64, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.481376004219
block_size_x=64, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.380812799931
block_size_x=64, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.351923197508
block_size_x=64, block_size_y=4, tile_size_x=1, tile_size_y=1, time=0.719257593155
block_size_x=64, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.55171200037
block_size_x=64, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.466758400202
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.568435204029
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.459654402733
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.394380801916
block_size_x=64, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.463052803278
block_size_x=64, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.36409599781
block_size_x=64, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.328998398781
block_size_x=64, block_size_y=8, tile_size_x=1, tile_size_y=1, time=0.73579518795
block_size_x=64, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.564575994015
block_size_x=64, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.472236800194
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.549024009705
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.438406395912
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.389945602417
block_size_x=64, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.455193603039
block_size_x=64, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.364051198959
block_size_x=64, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.375519996881
block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=1, time=0.798195195198
block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.588998401165
block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.49552000761
block_size_x=64, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.595462405682
block_size_x=64, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.460972803831
block_size_x=64, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.400672000647
block_size_x=64, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.465132802725
block_size_x=64, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.364627194405
block_size_x=128, block_size_y=2, tile_size_x=1, tile_size_y=1, time=0.729363203049
block_size_x=128, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.558815991879
block_size_x=128, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.466655993462
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.600819194317
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.460281592607
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.404908800125
block_size_x=128, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.478739196062
block_size_x=128, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.386668801308
block_size_x=128, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.385510402918
block_size_x=128, block_size_y=4, tile_size_x=1, tile_size_y=1, time=0.720915210247
block_size_x=128, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.550668799877
block_size_x=128, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.466937589645
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.564921605587
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.447974395752
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.394271999598
block_size_x=128, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.46233600378
block_size_x=128, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.365190398693
block_size_x=128, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.387827193737
block_size_x=128, block_size_y=8, tile_size_x=1, tile_size_y=1, time=0.762003195286
block_size_x=128, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.579007995129
block_size_x=128, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.486649608612
block_size_x=128, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.557331204414
block_size_x=128, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.443033593893
block_size_x=128, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.396070402861
block_size_x=128, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.457075202465
block_size_x=128, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.369555193186
best performing configuration: block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.322547197342
</pre></div></div>
</div>
<p>We can see that the number of kernel configurations tried by the Kernel Tuner is growing rather quickly. Also, the best performing configuration quite a bit faster than the best kernel before we started optimizing. On our GTX Titan X, the execution time went from 0.72 ms to 0.53 ms, a performance improvement of 26%!</p>
<p>Note that the thread block dimensions for this kernel configuration are also different. Without optimizations the best performing kernel used a thread block of 32x2, after we’ve added tiling the best performing kernel uses thread blocks of size 64x4, which is four times as many threads! Also the amount of work increased with tiling factors 2 in the x-direction and 4 in the y-direction, increasing the amount of work per thread block by a factor of 8. The difference in the area processed per
thread block between the naive and the tiled kernel is a factor 32.</p>
<p>However, there are actually several kernel configurations that come close. The following Python code prints all instances with an execution time within 5% of the best performing configuration.</p>
</section>
<section id="Using-the-best-parameters-in-a-production-run">
<h2>Using the best parameters in a production run<a class="headerlink" href="#Using-the-best-parameters-in-a-production-run" title="Permalink to this heading">¶</a></h2>
<p>Now that we have determined which parameters are the best for our problems we can use them to simulate the heat diffusion problem. There are several ways to do so depending on the host language you wish to use.</p>
<section id="Python-run">
<h3>Python run<a class="headerlink" href="#Python-run" title="Permalink to this heading">¶</a></h3>
<p>To use the optimized parameters in a python run, we simply have to modify the kernel code to specify which value to use for the block and tile size. There are of course many different ways to achieve this. In simple cases on can define a dictionary of values and replace the string <code class="docutils literal notranslate"><span class="pre">block_size_i</span></code> and <code class="docutils literal notranslate"><span class="pre">tile_size_j</span></code> by their values.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pycuda.autoinit</span>

<span class="c1"># define the optimal parameters</span>
<span class="n">size</span> <span class="o">=</span> <span class="p">[</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">threads</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># create a dict of fixed parameters</span>
<span class="n">fixed_params</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">fixed_params</span><span class="p">[</span><span class="s1">&#39;block_size_x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">threads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fixed_params</span><span class="p">[</span><span class="s1">&#39;block_size_y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">threads</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># select the kernel to use</span>
<span class="n">kernel_string</span> <span class="o">=</span> <span class="n">kernel_string_shared</span>

<span class="c1"># replace the block/tile size</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">fixed_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">kernel_string</span> <span class="o">=</span> <span class="n">kernel_string</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>We also need to determine the size of the grid</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># for regular and shared kernel</span>
<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">threads</span><span class="p">,</span><span class="n">size</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<p>We can then transfer the data initial condition on the two gpu arrays as well as compile the code and get the function we want to use.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#allocate GPU memory</span>
<span class="n">u_old</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>
<span class="n">u_new</span> <span class="o">=</span> <span class="n">gpuarray</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">field</span><span class="p">)</span>

<span class="c1"># compile the kernel</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">SourceModule</span><span class="p">(</span><span class="n">kernel_string</span><span class="p">)</span>
<span class="n">diffuse_kernel</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s2">&quot;diffuse_kernel&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We now just have to use the kernel with these optimized parameters to run the simulation</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#call the GPU kernel a 1000 times and measure performance</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">diffuse_kernel</span><span class="p">(</span><span class="n">u_new</span><span class="p">,</span> <span class="n">u_old</span><span class="p">,</span> <span class="n">block</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">threads</span><span class="p">),</span> <span class="n">grid</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
    <span class="n">diffuse_kernel</span><span class="p">(</span><span class="n">u_old</span><span class="p">,</span> <span class="n">u_new</span><span class="p">,</span> <span class="n">block</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">threads</span><span class="p">),</span> <span class="n">grid</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">driver</span><span class="o">.</span><span class="n">Context</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;1000 steps of diffuse on a </span><span class="si">%d</span><span class="s2"> x </span><span class="si">%d</span><span class="s2"> grid took&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span><span class="n">ny</span><span class="p">),</span> <span class="p">(</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="s2">&quot;ms.&quot;</span><span class="p">)</span>

<span class="c1">#copy the result from the GPU to Python for plotting</span>
<span class="n">gpu_result</span> <span class="o">=</span> <span class="n">u_old</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">gpu_result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
1000 steps of diffuse on a 4096 x 4096 grid took 618.2231903076172 ms.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x7f887c3d2358&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_use_optparam_48_2.png" src="_images/diffusion_use_optparam_48_2.png" />
</div>
</div>
</section>
<section id="C-run">
<h3>C run<a class="headerlink" href="#C-run" title="Permalink to this heading">¶</a></h3>
<p>If you wish to incorporate the optimized parameters in the kernel and use it in a C run you can use <code class="docutils literal notranslate"><span class="pre">ifndef</span></code> statement at the begining of the kerenel as demonstrated in the psedo code below.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>

<span class="s2">#ifndef block_size_x</span>
<span class="s2">    #define block_size_x &lt;insert optimal value&gt;</span>
<span class="s2">#endif</span>

<span class="s2">#ifndef block_size_y</span>
<span class="s2">    #define block_size_y &lt;insert optimal value&gt;</span>
<span class="s2">#endif</span>

<span class="s2">#define nx </span><span class="si">%d</span>
<span class="s2">#define ny </span><span class="si">%d</span>
<span class="s2">#define dt 0.225f</span>
<span class="s2">__global__ void diffuse_kernel(float *u_new, float *u) {</span>
<span class="s2">    ......</span>
<span class="s2">    }</span>

<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This kernel can be used during the tuning since the kernel tuner will prepend <code class="docutils literal notranslate"><span class="pre">#define</span></code> statements to the kernel. As a result the <code class="docutils literal notranslate"><span class="pre">#ifndef</span></code> will be bypassed during the tuning. However the same kernel will work just fine on its own in a larger program.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016-2024, Ben van Werkhoven, Alessio Sclocco, Stijn Heldens, Floris-Jan Willemsen, Willem-Jan Palenstijn, Bram Veenboer and Richard Schoonhoven.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>