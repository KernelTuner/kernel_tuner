

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>API Documentation &mdash; Kernel Tuner 0.3.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Design documentation" href="design.html" />
    <link rel="prev" title="Tuning Host Code" href="hostcode.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Kernel Tuner
          

          
          </a>

          
            
            
              <div class="version">
                0.3.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Tutorial: From physics to tuned GPU kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Kernel Tuner Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_multiplication.html">Matrix multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Kernel Correctness Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="hostcode.html">Tuning Host Code</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="design.html">Design documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribution guide</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Kernel Tuner</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>API Documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            
              <a href="https://github.com/benvanwerkhoven/kernel_tuner/blob/master/doc/source/user-api.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="toctree-wrapper compound">
</div>
<div class="section" id="api-documentation">
<span id="details"></span><h1>API Documentation<a class="headerlink" href="#api-documentation" title="Permalink to this headline">¶</a></h1>
<p>This file provides all the details you need about how to call the Kernel Tuner’s functions, including all the optional arguments.</p>
<dl class="py function">
<dt class="sig sig-object py" id="kernel_tuner.tune_kernel">
<span class="sig-prename descclassname"><span class="pre">kernel_tuner.</span></span><span class="sig-name descname"><span class="pre">tune_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_string</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">problem_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arguments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tune_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_x</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_z</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restrictions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">answer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">atol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verify</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">platform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texmem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compiler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compiler_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quiet</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strategy_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.tune_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Tune a CUDA kernel given a set of tunable parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_name</strong> (<em>string</em>) – The name of the kernel in the code.</p></li>
<li><p><strong>kernel_source</strong> (<em>string</em><em> or </em><em>list and/or callable</em>) – <p>The CUDA, OpenCL, or C kernel code.
It is allowed for the code to be passed as a string, a filename, a function
that returns a string of code, or a list when the code needs auxilliary files.</p>
<p>To support combined host and device code tuning, a list of
filenames can be passed. The first file in the list should be the
file that contains the host code. The host code is assumed to
include or read in any of the files in the list beyond the first.
The tunable parameters can be used within all files.</p>
<p>Another alternative is to pass a code generating function.
The purpose of this is to support the use of code generating
functions that generate the kernel code based on the specific
parameters. This function should take one positional argument,
which will be used to pass a dict containing the parameters.
The function should return a string with the source code for
the kernel.</p>
</p></li>
<li><p><strong>lang</strong> (<em>string</em>) – Specifies the language used for GPU kernels. The kernel_tuner
automatically detects the language, but if it fails, you may specify
the language using this argument, currently supported: “CUDA”,
“OpenCL”, or “C”.</p></li>
<li><p><strong>problem_size</strong> (<em>string</em><em>, </em><em>int</em><em>, or </em><em>tuple</em><em>(</em><em>int</em><em> or </em><em>string</em><em>, </em><em>.</em><em>)</em>) – <p>An int or string, or 1,2,3-dimensional tuple
containing the size from which the grid dimensions of the kernel
will be computed.</p>
<p>Do not divide the problem_size yourself by the thread block sizes.
The Kernel Tuner does this for you based on tunable parameters,
called “block_size_x”, “block_size_y”, and “block_size_z”.
If more or different parameters divide the grid dimensions use
grid_div_x/y/z options to specify this.</p>
<p>You are allowed to use a string to specify the problem
size. Within a string you are allowed to write Python
arithmetic and use the names of tunable parameters as variables
in these expressions.
The Kernel Tuner will replace instances of the tunable parameters
with their current value when computing the grid dimensions.
See the reduction CUDA example for an example use of this feature.</p>
</p></li>
<li><p><strong>arguments</strong> (<em>list</em>) – A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for scalars.</p></li>
<li><p><strong>grid_div_x</strong> (<em>list</em>) – <p>A list of names of the parameters whose values divide
the grid dimensions in the x-direction.
The product of all grid divisor expressions is computed before dividing
the problem_size in that dimension. Also note that the divison is treated
as a float divison and resulting grid dimensions will be rounded up to
the nearest integer number.</p>
<p>Arithmetic expressions can be
used if necessary inside the string containing a parameter name. For
example, in some cases you may want to divide the problem size in the
x-dimension with the number of warps rather than the number of threads
in a block, in such cases one could use [“block_size_x/32”].</p>
<p>If not supplied, [“block_size_x”] will be used by default, if you do not
want any grid x-dimension divisors pass an empty list.</p>
</p></li>
<li><p><strong>grid_div_y</strong> (<em>list</em>) – A list of names of the parameters whose values divide
the grid dimensions in the y-direction, [“block_size_y”] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</p></li>
<li><p><strong>grid_div_z</strong> (<em>list</em>) – A list of names of the parameters whose values divide
the grid dimensions in the z-direction, [“block_size_z”] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</p></li>
<li><p><strong>smem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em>)</em>) – CUDA-specific feature for specifying shared memory options
to the kernel. At the moment only ‘size’ is supported, but setting the
shared memory configuration on Kepler GPUs for example could be added
in the future. Size should denote the number of bytes for to use when
dynamically allocating shared memory.</p></li>
<li><p><strong>cmem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em>)</em>) – CUDA-specific feature for specifying constant memory
arguments to the kernel. In OpenCL these are handled as normal
kernel arguments, but in CUDA you can copy to a symbol. The way you
specify constant memory arguments is by passing a dictionary with
strings containing the constant memory symbol name together with numpy
objects in the same way as normal kernel arguments.</p></li>
<li><p><strong>texmem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em> or </em><em>dict</em><em>)</em>) – CUDA-specific feature for specifying texture memory
arguments to the kernel. You specify texture memory arguments by passing a
dictionary with strings containing the texture reference name together with
the texture contents. These contents can be either simply a numpy object,
or a dictionary containing the numpy object under the key ‘array’ plus the
configuration options ‘filter_mode’ (‘point’ or ‘linear), ‘address_mode’
(a list of ‘border’, ‘clamp’, ‘mirror’, ‘wrap’ per axis),
‘normalized_coordinates’ (True/False).</p></li>
<li><p><strong>block_size_names</strong> (<em>list</em><em>(</em><em>string</em><em>)</em>) – A list of strings that replace the defaults for the names
that denote the thread block dimensions. If not passed, the behavior
defaults to <code class="docutils literal notranslate"><span class="pre">[&quot;block_size_x&quot;,</span> <span class="pre">&quot;block_size_y&quot;,</span> <span class="pre">&quot;block_size_z&quot;]</span></code></p></li>
<li><p><strong>tune_params</strong> (<em>dict</em><em>( </em><em>string :</em><em> [</em><em>...</em><em>]</em>) – <p>A dictionary containing the parameter names as keys,
and lists of possible parameter settings as values.
The Kernel Tuner will try to compile and benchmark all possible
combinations of all possible values for all tuning parameters.
This typically results in a rather large search space of all
possible kernel configurations.</p>
<p>For each kernel configuration, each tuning parameter is
replaced at compile-time with its current value.
Currently, the Kernel Tuner uses the convention that the following
list of tuning parameters are used as thread block dimensions:</p>
<blockquote>
<div><ul>
<li><p>”block_size_x”   thread block (work group) x-dimension</p></li>
<li><p>”block_size_y”   thread block (work group) y-dimension</p></li>
<li><p>”block_size_z”   thread block (work group) z-dimension</p></li>
</ul>
</div></blockquote>
<p>Options for changing these defaults may be added later. If you
don’t want the thread block dimensions to be compiled in, you
may use the built-in variables blockDim.xyz in CUDA or the
built-in function get_local_size() in OpenCL instead.</p>
</p></li>
<li><p><strong>restrictions</strong> (<em>list</em>) – A list of strings containing boolean expression that
limit the search space in that they must be satisfied by the kernel
configuration. These expressions must be true for the configuration
to be part of the search space. For example:
restrictions=[“block_size_x==block_size_y*tile_size_y”] limits the
search to configurations where the block_size_x equals the product
of block_size_y and tile_size_y.
The default is None.</p></li>
<li><p><strong>answer</strong> (<em>list</em>) – A list of arguments, similar to what you pass to arguments,
that contains the expected output of the kernel after it has executed
and contains None for each argument that is input-only. The expected
output of the kernel will then be used to verify the correctness of
each kernel in the parameter space before it will be benchmarked.</p></li>
<li><p><strong>atol</strong> (<em>float</em>) – The maximum allowed absolute difference between two elements
in the output and the reference answer, as passed to numpy.allclose().
Ignored if you have not passed a reference answer. Default value is
1e-6, that is 0.000001.</p></li>
<li><p><strong>verify</strong> (<em>func</em><em>(</em><em>ref</em><em>, </em><em>ans</em><em>, </em><em>atol=None</em><em>)</em>) – <p>Python function used for output verification. By default,
numpy.allclose is used for output verification, if this does not suit
your application, you can pass a different function here.</p>
<p>The function is expected to have two positional arguments. The first
is the reference result, the second is the output computed by the
kernel being verified. The types of these arguments depends on the
type of the output arguments you are verifying. The function may also
have an optional argument named atol, to which the value will be
passed that was specified using the atol option to tune_kernel.
The function should return True when the output passes the test, and
False when the output fails the test.</p>
</p></li>
<li><p><strong>strategy</strong> – <p>Specify the strategy to use for searching through the
parameter space, choose from:</p>
<blockquote>
<div><ul>
<li><p>”brute_force” (default),</p></li>
<li><p>”random_sample”, specify: <em>sample_fraction</em>,</p></li>
<li><p>”minimize” or “basinhopping”, specify: <em>method</em>,</p></li>
<li><p>”diff_evo”, specify: <em>method</em>.</p></li>
<li><p>”genetic_algorithm”</p></li>
<li><p>”pso”</p></li>
<li><p>”firefly_algorithm”</p></li>
<li><p>”simulated_annealing”</p></li>
<li><p>”bayes_opt”</p></li>
</ul>
</div></blockquote>
<p>”brute_force” is the default and iterates over the entire search
space.</p>
<p>”random_sample” can be used to only benchmark a fraction of the
search space, specify a <em>sample_fraction</em> in the interval [0, 1].</p>
<p>”minimize” and “basinhopping” strategies use minimizers to
limit the search through the parameter space.</p>
<p>”diff_evo” uses differential evolution.</p>
<p>”genetic_algorithm” implements a Genetic Algorithm, default
setting uses a population size of 20 for 100 generations.</p>
<p>”pso” implements Particle Swarm Optimization, using the default
setting of 20 particles for 100 iterations.</p>
<p>”firefly_algorithm” implements the Firefly Algorithm, using 20
fireflies for 100 iterations.</p>
<p>”simulated_annealing” uses Simulated Annealing.</p>
<p>”bayes_opt” uses Bayesian Optimization.</p>
</p></li>
<li><p><strong>strategy_options</strong> (<em>dict</em>) – <p>A dict with options for the tuning strategy</p>
<p>Example usage:</p>
<blockquote>
<div><ul>
<li><p>strategy=”basinhopping”,
strategy_options={“method”: “BFGS”,</p>
<blockquote>
<div><p>”maxiter”: 100,
“T”: 1.0}</p>
</div></blockquote>
</li>
<li><p>strategy=”diff_evo”,
strategy_options={“method”: “best1bin”,</p>
<blockquote>
<div><p>”popsize”: 20}</p>
</div></blockquote>
</li>
<li><p>strategy=”genetic_algorithm”,
strategy_options={“method”: “uniform”,</p>
<blockquote>
<div><p>”popsize”: 20,
“maxiter”: 100}</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>strategy=”minimize” and strategy=”basinhopping”, support the following
options for “method”:
“Nelder-Mead”, “Powell”, “CG”, “BFGS”, “L-BFGS-B”,
“TNC”, “COBYLA”, or “SLSQP”. It is also possible to pass a function
that implements a custom minimization strategy.
The default is “L-BFGS-B”.
strategy=”basinhopping” also supports “T”, which is 1.0 by default.</p>
<p>strategy=”diff_evo” supports the following creation “method” options:
“best1bin”, “best1exp”, “rand1exp”, “randtobest1exp”, “best2exp”,
“rand2exp”, “randtobest1bin”, “best2bin”, “rand2bin”, “rand1bin”.
The default is “best1bin”.</p>
<p>strategy=”genetic_algorithm” uses “method” to select the crossover
method, options are: “single_point”, “two_point”, “uniform”, and
“disruptive_uniform”.
The default is “uniform”.
Also “mutation_chance” can be set to control the chance of a mutation,
which is separately evaluated for each dimension. For example, set
to 100 for a probability of 0.01 of a mutation per tunable parameter.</p>
<p>strategy=”random_sample” supports “fraction” to specify
the fraction of the search space to sample in the interval [0,1].</p>
<p>strategy=”firefly_algorithm” supports the following parameters:
B0 = 1.0, gamma = 1.0, alpha = 0.20.</p>
<p>strategy=”simulated_annealing” supports parameters:
T = 1.0, T_min = 0.001, alpha = 0.9.</p>
<p>strategy=”bayes_opt” supports acquisition methods: “poi” (default),
“ei”, “ucb”. And parameters, popsize (initial random guesses),
maxiter, alpha, kappa, xi.</p>
<p>”maxiter” is supported by “minimize”, “basinhopping”, “diff_evo”
“firefly_algorithm”, “pso”, “genetic_algorithm”, “bayes_opt”. Note
that maxiter generally refers to iterations of the strategy, not
the maximum number of function evaluations.</p>
</p></li>
<li><p><strong>iterations</strong> (<em>int</em>) – The number of times a kernel should be executed and
its execution time measured when benchmarking a kernel, 7 by default.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – <p>Sets whether or not to report about configurations that
were skipped during the search. This could be due to several reasons:</p>
<blockquote>
<div><ul>
<li><p>kernel configuration fails one or more restrictions</p></li>
<li><p>too many threads per thread block</p></li>
<li><p>too much shared memory used by the kernel</p></li>
<li><p>too many resources requested for launch</p></li>
</ul>
</div></blockquote>
<p>verbose is False by default.</p>
</p></li>
<li><p><strong>cache</strong> (<em>string</em>) – filename for caching/logging benchmarked instances
filename uses suffix “.json”
if the file exists it is read and tuning continues from this file</p></li>
<li><p><strong>device</strong> (<em>int</em>) – CUDA/OpenCL device to use, in case you have multiple
CUDA-capable GPUs or OpenCL devices you may use this to select one,
0 by default. Ignored if you are tuning host code by passing
lang=”C”.</p></li>
<li><p><strong>platform</strong> (<em>int</em>) – OpenCL platform to use, in case you have multiple
OpenCL platforms you may use this to select one,
0 by default. Ignored if not using OpenCL.</p></li>
<li><p><strong>quiet</strong> (<em>boolean</em>) – Control whether or not to print to the console which
device is being used, False by default</p></li>
<li><p><strong>compiler</strong> (<em>string</em>) – A string containing your preferred compiler,
only effective with lang=”C”.</p></li>
<li><p><strong>compiler_options</strong> (<em>list</em><em>(</em><em>string</em><em>)</em>) – A list of strings that specify compiler
options.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of dictionaries of all executed kernel configurations and their
execution times. And a dictionary with information about the environment
in which the tuning took place. This records device name, properties,
version info, and so on.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list(dict()), dict()</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="kernel_tuner.run_kernel">
<span class="sig-prename descclassname"><span class="pre">kernel_tuner.</span></span><span class="sig-name descname"><span class="pre">run_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_string</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">problem_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arguments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_x</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_z</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">platform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texmem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compiler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compiler_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quiet</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.run_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compile and run a single kernel</p>
<p>Compiles and runs a single kernel once, given a specific instance of the kernels tuning parameters.
However, instead of measuring execution time run_kernel returns the output of the kernel.
The output is returned as a list of numpy arrays that contains the state of all the kernel arguments
after execution on the GPU.</p>
<dl class="simple">
<dt>To summarize what this function will do for you in one call:</dt><dd><ul class="simple">
<li><p>Compile the kernel according to the set of parameters passed</p></li>
<li><p>Allocate GPU memory to hold all kernel arguments</p></li>
<li><p>Move the all data to the GPU</p></li>
<li><p>Execute the kernel on the GPU</p></li>
<li><p>Copy all data from the GPU back to the host and return it as a list of Numpy arrays</p></li>
</ul>
</dd>
</dl>
<p>This function was added to the Kernel Tuner mostly to allow easy testing for kernel correctness.
On purpose, the interface is a lot like <cite>tune_kernel()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_name</strong> (<em>string</em>) – The name of the kernel in the code.</p></li>
<li><p><strong>kernel_source</strong> (<em>string</em><em> or </em><em>list and/or callable</em>) – <p>The CUDA, OpenCL, or C kernel code.
It is allowed for the code to be passed as a string, a filename, a function
that returns a string of code, or a list when the code needs auxilliary files.</p>
<p>To support combined host and device code tuning, a list of
filenames can be passed. The first file in the list should be the
file that contains the host code. The host code is assumed to
include or read in any of the files in the list beyond the first.
The tunable parameters can be used within all files.</p>
<p>Another alternative is to pass a code generating function.
The purpose of this is to support the use of code generating
functions that generate the kernel code based on the specific
parameters. This function should take one positional argument,
which will be used to pass a dict containing the parameters.
The function should return a string with the source code for
the kernel.</p>
</p></li>
<li><p><strong>lang</strong> (<em>string</em>) – Specifies the language used for GPU kernels. The kernel_tuner
automatically detects the language, but if it fails, you may specify
the language using this argument, currently supported: “CUDA”,
“OpenCL”, or “C”.</p></li>
<li><p><strong>problem_size</strong> (<em>string</em><em>, </em><em>int</em><em>, or </em><em>tuple</em><em>(</em><em>int</em><em> or </em><em>string</em><em>, </em><em>.</em><em>)</em>) – <p>An int or string, or 1,2,3-dimensional tuple
containing the size from which the grid dimensions of the kernel
will be computed.</p>
<p>Do not divide the problem_size yourself by the thread block sizes.
The Kernel Tuner does this for you based on tunable parameters,
called “block_size_x”, “block_size_y”, and “block_size_z”.
If more or different parameters divide the grid dimensions use
grid_div_x/y/z options to specify this.</p>
<p>You are allowed to use a string to specify the problem
size. Within a string you are allowed to write Python
arithmetic and use the names of tunable parameters as variables
in these expressions.
The Kernel Tuner will replace instances of the tunable parameters
with their current value when computing the grid dimensions.
See the reduction CUDA example for an example use of this feature.</p>
</p></li>
<li><p><strong>arguments</strong> (<em>list</em>) – A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for scalars.</p></li>
<li><p><strong>grid_div_x</strong> (<em>list</em>) – <p>A list of names of the parameters whose values divide
the grid dimensions in the x-direction.
The product of all grid divisor expressions is computed before dividing
the problem_size in that dimension. Also note that the divison is treated
as a float divison and resulting grid dimensions will be rounded up to
the nearest integer number.</p>
<p>Arithmetic expressions can be
used if necessary inside the string containing a parameter name. For
example, in some cases you may want to divide the problem size in the
x-dimension with the number of warps rather than the number of threads
in a block, in such cases one could use [“block_size_x/32”].</p>
<p>If not supplied, [“block_size_x”] will be used by default, if you do not
want any grid x-dimension divisors pass an empty list.</p>
</p></li>
<li><p><strong>grid_div_y</strong> (<em>list</em>) – A list of names of the parameters whose values divide
the grid dimensions in the y-direction, [“block_size_y”] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</p></li>
<li><p><strong>grid_div_z</strong> (<em>list</em>) – A list of names of the parameters whose values divide
the grid dimensions in the z-direction, [“block_size_z”] by default.
If you do not want to divide the problem_size, you should pass an empty list.
See grid_div_x for more details.</p></li>
<li><p><strong>smem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em>)</em>) – CUDA-specific feature for specifying shared memory options
to the kernel. At the moment only ‘size’ is supported, but setting the
shared memory configuration on Kepler GPUs for example could be added
in the future. Size should denote the number of bytes for to use when
dynamically allocating shared memory.</p></li>
<li><p><strong>cmem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em>)</em>) – CUDA-specific feature for specifying constant memory
arguments to the kernel. In OpenCL these are handled as normal
kernel arguments, but in CUDA you can copy to a symbol. The way you
specify constant memory arguments is by passing a dictionary with
strings containing the constant memory symbol name together with numpy
objects in the same way as normal kernel arguments.</p></li>
<li><p><strong>texmem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em> or </em><em>dict</em><em>)</em>) – CUDA-specific feature for specifying texture memory
arguments to the kernel. You specify texture memory arguments by passing a
dictionary with strings containing the texture reference name together with
the texture contents. These contents can be either simply a numpy object,
or a dictionary containing the numpy object under the key ‘array’ plus the
configuration options ‘filter_mode’ (‘point’ or ‘linear), ‘address_mode’
(a list of ‘border’, ‘clamp’, ‘mirror’, ‘wrap’ per axis),
‘normalized_coordinates’ (True/False).</p></li>
<li><p><strong>block_size_names</strong> (<em>list</em><em>(</em><em>string</em><em>)</em>) – A list of strings that replace the defaults for the names
that denote the thread block dimensions. If not passed, the behavior
defaults to <code class="docutils literal notranslate"><span class="pre">[&quot;block_size_x&quot;,</span> <span class="pre">&quot;block_size_y&quot;,</span> <span class="pre">&quot;block_size_z&quot;]</span></code></p></li>
<li><p><strong>params</strong> (<em>dict</em><em>( </em><em>string: int</em><em> )</em>) – A dictionary containing the tuning parameter names as keys
and a single value per tuning parameter as values.</p></li>
<li><p><strong>device</strong> (<em>int</em>) – CUDA/OpenCL device to use, in case you have multiple
CUDA-capable GPUs or OpenCL devices you may use this to select one,
0 by default. Ignored if you are tuning host code by passing
lang=”C”.</p></li>
<li><p><strong>platform</strong> (<em>int</em>) – OpenCL platform to use, in case you have multiple
OpenCL platforms you may use this to select one,
0 by default. Ignored if not using OpenCL.</p></li>
<li><p><strong>quiet</strong> (<em>boolean</em>) – Control whether or not to print to the console which
device is being used, False by default</p></li>
<li><p><strong>compiler</strong> (<em>string</em>) – A string containing your preferred compiler,
only effective with lang=”C”.</p></li>
<li><p><strong>compiler_options</strong> (<em>list</em><em>(</em><em>string</em><em>)</em>) – A list of strings that specify compiler
options.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of numpy arrays, similar to the arguments passed to this
function, containing the output after kernel execution.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="kernel_tuner.wrappers.cpp">
<span class="sig-prename descclassname"><span class="pre">kernel_tuner.wrappers.</span></span><span class="sig-name descname"><span class="pre">cpp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">function_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">convert_to_array</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.wrappers.cpp" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a wrapper to call C++ functions from Python</p>
<p>This function allows Kernel Tuner to call templated C++ functions
that use primitive data types (double, float, int, …).</p>
<p>There is support to convert function arguments from plain pointers
to array references. If this is needed, there should be a True value
in convert_to_array in the location corresponding to the location in
the args array.</p>
<p>For example, a Numpy array argument of type float64 and length 10
will be cast using:
<code class="docutils literal notranslate"><span class="pre">*reinterpret_cast&lt;double(*)[10]&gt;(arg)</span></code>
which allows it to be used to call a C++ that is defined as:
<code class="docutils literal notranslate"><span class="pre">template&lt;typename</span> <span class="pre">T,</span> <span class="pre">int</span> <span class="pre">s&gt;void</span> <span class="pre">my_function(T</span> <span class="pre">(&amp;arg)[s],</span> <span class="pre">...)</span></code></p>
<p>Arrays of size 1 will be converted to simple non-array references.
False indicates that no conversion is performed. Conversion
is only support for numpy array arguments. If convert_to_array is
passed it should have the same length as the args array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>function_name</strong> (<em>string</em>) – A string containing the name of the C++ function
to be wrapped</p></li>
<li><p><strong>kernel_source</strong> (<em>string</em><em> or </em><em>callable</em>) – One of the sources for the kernel, could be a
function that generates the kernel code, a string containing a filename
that points to the kernel source, or just a string that contains the code.</p></li>
<li><p><strong>args</strong> (<em>list</em>) – A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for scalars.</p></li>
<li><p><strong>convert_to_array</strong> (<em>list</em><em> (</em><em>True</em><em> or </em><em>False</em><em>)</em>) – A list of same length as args, containing
True or False values indicating whether the corresponding argument
in args should be cast to a reference to an array or not.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A string containing the orignal code extended with the wrapper
function. The wrapper has “extern C” binding and can be passed to
other Kernel Tuner functions, for example run_kernel with lang=”C”.
The name of the wrapper function will be the name of the function with
a “_wrapper” postfix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="design.html" class="btn btn-neutral float-right" title="Design documentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="hostcode.html" class="btn btn-neutral float-left" title="Tuning Host Code" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2016, Ben van Werkhoven.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>