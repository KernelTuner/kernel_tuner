<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial: From physics to tuned GPU kernels &mdash; Kernel Tuner 0.4.3 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="contents.html" class="icon icon-home"> Kernel Tuner
          </a>
              <div class="version">
                0.4.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Kernel Tuner</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_multiplication.html">Matrix multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Kernel Tuner Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cache_files.html">Cache files</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Correctness Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="hostcode.html">Tuning Host Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="structs.html">Using structs</a></li>
<li class="toctree-l1"><a class="reference internal" href="templates.html">Templated kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics and Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="observers.html">Observers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user-api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="vocabulary.html">Parameter Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="design.html">Design documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribution guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="contents.html">Kernel Tuner</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="contents.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Tutorial: From physics to tuned GPU kernels</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/KernelTuner/kernel_tuner/blob/master/doc/source/diffusion_opencl.ipynb" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Tutorial:-From-physics-to-tuned-GPU-kernels">
<h1>Tutorial: From physics to tuned GPU kernels<a class="headerlink" href="#Tutorial:-From-physics-to-tuned-GPU-kernels" title="Permalink to this heading">¶</a></h1>
<p>This tutorial is designed to show you the whole process starting from modeling a physical process to a Python implementation to creating optimized and auto-tuned GPU application using Kernel Tuner.</p>
<p>In this tutorial, we will use <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion">diffusion</a> as an example application.</p>
<p>We start with modeling the physical process of diffusion, for which we create a simple numerical implementation in Python. Then we create an OpenCL kernel that performs the same computation, but on the GPU. Once we have a OpenCL kernel, we start using the Kernel Tuner for auto-tuning our GPU application. And finally, we’ll introduce a few code optimizations to our OpenCL kernel that will improve performance, but also add more parameters to tune on using the Kernel Tuner.</p>
<div class="admonition note">
<p><strong>Note:</strong> If you are reading this tutorial on the Kernel Tuner’s documentation pages, note that you can actually run this tutorial as a Jupyter Notebook. Just clone the Kernel Tuner’s <a class="reference external" href="http://github.com/benvanwerkhoven/kernel_tuner">GitHub repository</a>. Install using <em>pip install .[tutorial,opencl]</em> and you’re ready to go! You can start the tutorial by typing “jupyter notebook” in the “kernel_tuner/doc/source” directory.</p>
</div>
<section id="Diffusion">
<h2>Diffusion<a class="headerlink" href="#Diffusion" title="Permalink to this heading">¶</a></h2>
<p>Put simply, diffusion is the redistribution of something from a region of high concentration to a region of low concentration without bulk motion. The concept of diffusion is widely used in many fields, including physics, chemistry, biology, and many more.</p>
<p>Suppose that we take a metal sheet, in which the temperature is exactly equal to one degree everywhere in the sheet. Now if we were to heat a number of points on the sheet to a very high temperature, say a thousand degrees, in an instant by some method. We could see the heat diffuse from these hotspots to the cooler areas. We are assuming that the metal does not melt. In addition, we will ignore any heat loss from radiation or other causes in this example.</p>
<p>We can use the <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion_equation">diffusion equation</a> to model how the heat diffuses through our metal sheet:</p>
<p><span class="math">\begin{equation*}
\frac{\partial u}{\partial t}= D \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right)
\end{equation*}</span></p>
<p>Where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> represent the spatial descretization of our 2D domain, <span class="math notranslate nohighlight">\(u\)</span> is the quantity that is being diffused, <span class="math notranslate nohighlight">\(t\)</span> is the descretization in time, and the constant <span class="math notranslate nohighlight">\(D\)</span> determines how fast the diffusion takes place.</p>
<p>In this example, we will assume a very simple descretization of our problem. We assume that our 2D domain has <span class="math notranslate nohighlight">\(nx\)</span> equi-distant grid points in the x-direction and <span class="math notranslate nohighlight">\(ny\)</span> equi-distant grid points in the y-direction. Be sure to execute every cell as you read through this document, by selecting it and pressing <strong>shift+enter</strong>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>nx = 1024
ny = 1024
</pre></div>
</div>
</div>
<p>This results in a constant distance of <span class="math notranslate nohighlight">\(\delta x\)</span> between all grid points in the <span class="math notranslate nohighlight">\(x\)</span> dimension. Using central differences, we can numerically approximate the derivative for a given point <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<p><span class="math">\begin{equation*}
\left. \frac{\partial^2 u}{\partial x^2} \right|_{x_{i}} \approx \frac{u_{x_{i+1}}-2u_{{x_i}}+u_{x_{i-1}}}{(\delta x)^2}
\end{equation*}</span></p>
<p>We do the same for the partial derivative in <span class="math notranslate nohighlight">\(y\)</span>:</p>
<p><span class="math">\begin{equation*}
\left. \frac{\partial^2 u}{\partial y^2} \right|_{y_{i}} \approx \frac{u_{y_{i+1}}-2u_{y_{i}}+u_{y_{i-1}}}{(\delta y)^2}
\end{equation*}</span></p>
<p>If we combine the above equations, we can obtain a numerical estimation for the temperature field of our metal sheet in the next time step, using <span class="math notranslate nohighlight">\(\delta t\)</span> as the time between time steps. But before we do, we also simplify the expression a little bit, because we’ll assume that <span class="math notranslate nohighlight">\(\delta x\)</span> and <span class="math notranslate nohighlight">\(\delta y\)</span> are always equal to 1.</p>
<p><span class="math">\begin{equation*}
u'_{x,y} = u_{x,y} + \delta t \times \left( \left( u_{x_{i+1},y}-2u_{{x_i},y}+u_{x_{i-1},y} \right) + \left( u_{x,y_{i+1}}-2u_{x,y_{i}}+u_{x,y_{i-1}} \right) \right)
\end{equation*}</span></p>
<p>In this formula <span class="math notranslate nohighlight">\(u'_{x,y}\)</span> refers to the temperature field at the time <span class="math notranslate nohighlight">\(t + \delta t\)</span>. As a final step, we further simplify this equation to:</p>
<p><span class="math">\begin{equation*}
u'_{x,y} = u_{x,y} + \delta t \times \left( u_{x,y_{i+1}}+u_{x_{i+1},y}-4u_{{x_i},y}+u_{x_{i-1},y}+u_{x,y_{i-1}} \right)
\end{equation*}</span></p>
</section>
<section id="Python-implementation">
<h2>Python implementation<a class="headerlink" href="#Python-implementation" title="Permalink to this heading">¶</a></h2>
<p>We can create a Python function that implements the numerical approximation defined in the above equation. For simplicity we’ll use the assumption of a free boundary condition.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def diffuse(field, dt=0.225):
    field[1:nx-1,1:ny-1] = field[1:nx-1,1:ny-1] + dt * (
        field[1:nx-1,2:ny]+field[2:nx,1:ny-1]-4*field[1:nx-1,1:ny-1]+
        field[0:nx-2,1:ny-1]+field[1:nx-1,0:ny-2] )
    return field
</pre></div>
</div>
</div>
<p>To give our Python function a test run, we will now do some imports and generate the input data for the initial conditions of our metal sheet with a few very hot points. We’ll also make two plots, one after a thousand time steps, and a second plot after another two thousand time steps. Do note that the plots are using different ranges for the colors. Also, executing the following cell may take a little while.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#do the imports we need
import numpy
from matplotlib import pyplot
%matplotlib inline

#setup initial conditions
def get_initial_conditions(nx, ny):
    field = numpy.ones((ny, nx)).astype(numpy.float32)
    field[numpy.random.randint(0,nx,size=10), numpy.random.randint(0,ny,size=10)] = 1e3
    return field
field = get_initial_conditions(nx, ny)

#run the diffuse function a 1000 times and another 2000 times and make plots
fig, (ax1, ax2) = pyplot.subplots(1,2)
for i in range(1000):
    field = diffuse(field)
ax1.imshow(field)
for i in range(2000):
    field = diffuse(field)
ax2.imshow(field)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.image.AxesImage at 0x2aab1de088d0&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_opencl_7_1.png" src="_images/diffusion_opencl_7_1.png" />
</div>
</div>
<p>Now let’s take a quick look at the execution time of our diffuse function. Before we do, we also copy the current state of the metal sheet to be able to restart the computation from this state.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

#save the current field for later use
field_copy = numpy.copy(field)

#run another 1000 steps of the diffuse function and measure the time
for i in range(1000):
    field = diffuse(field)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 4.01 s, sys: 140 ms, total: 4.16 s
Wall time: 3.98 s
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>pyplot.imshow(field)
pyplot.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_opencl_10_0.png" src="_images/diffusion_opencl_10_0.png" />
</div>
</div>
</section>
<section id="Computing-on-the-GPU">
<h2>Computing on the GPU<a class="headerlink" href="#Computing-on-the-GPU" title="Permalink to this heading">¶</a></h2>
<p>The next step in this tutorial is to implement a GPU kernel that will allow us to run our problem on the GPU. We store the kernel code in a Python string, because we can directly compile and run the kernel from Python. In this tutorial, we’ll use the OpenCL programming model to implement our kernels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def get_kernel_string(nx, ny):
    return &quot;&quot;&quot;
    #define nx %d
    #define ny %d
    #define dt 0.225f
    __kernel void diffuse_kernel(global float *u_new, global float *u) {
        unsigned x = get_group_id(0) * block_size_x + get_local_id(0);
        unsigned y = get_group_id(1) * block_size_y + get_local_id(1);

        if (x&gt;0 &amp;&amp; x&lt;nx-1 &amp;&amp; y&gt;0 &amp;&amp; y&lt;ny-1) {
            u_new[y*nx+x] = u[y*nx+x] + dt * (
                u[(y+1)*nx+x]+u[y*nx+x+1]-4.0f*u[y*nx+x]+u[y*nx+x-1]+u[(y-1)*nx+x]);
        }
    }
    &quot;&quot;&quot; % (nx, ny)
kernel_string = get_kernel_string(nx, ny)
</pre></div>
</div>
</div>
<p>The above OpenCL kernel parallelizes the work such that every grid point will be processed by a different OpenCL thread. Therefore, the kernel is executed by a 2D grid of threads, which are grouped together into 2D thread blocks. The specific thread block dimensions we choose are not important for the result of the computation in this kernel. But as we will see will later, they will have an impact on performance.</p>
<p>In this kernel we are using two, currently undefined, compile-time constants for <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code>, because we will auto tune these parameters later. It is often needed for performance to fix the thread block dimensions at compile time, because the compiler can unroll loops that iterate using the block size, or because you need to allocate shared memory using the thread block dimensions.</p>
<p>The next bit of Python code initializes PyOpenCL, and makes preparations so that we can call the OpenCL kernel to do the computation on the GPU as we did earlier in Python.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pyopencl as cl

#initialize OpenCL and get compute capability needed for compilation
ctx = cl.create_some_context()
mf = cl.mem_flags

#reserve host memory
a_h = field_copy.astype(numpy.float32)

#allocate GPU memory (and copy from host buffer)
a_d = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_h)
b_d = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_h)

#setup thread block dimensions and compile the kernel
threads = (16,16)
kernel_src = &quot;#define block_size_x %d\n#define block_size_y %d&quot; % threads \
    + get_kernel_string(nx, ny)
prg = cl.Program(ctx, kernel_src).build()
</pre></div>
</div>
</div>
<p>The above code is a bit of boilerplate we need to compile a kernel using PyOpenCL. We’ve also, for the moment, fixed the thread block dimensions at 16 by 16. These dimensions serve as our initial guess for what a good performing pair of thread block dimensions could look like.</p>
<p>Now that we’ve setup everything, let’s see how long the computation would take using the GPU.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>queue = cl.CommandQueue(ctx)

def run_gpu():
    for i in range(500):
        prg.diffuse_kernel(
            queue, [nx, ny], threads, b_d, a_d)
        prg.diffuse_kernel(
            queue, [nx, ny], threads, a_d, b_d)

%time run_gpu()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 444 ms, sys: 154 ms, total: 598 ms
Wall time: 985 ms
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#copy the result from the GPU to Python for plotting
cl.enqueue_copy(queue, a_h, a_d)
queue.finish()

fig, (ax1, ax2) = pyplot.subplots(1,2)
ax1.imshow(a_h)
ax1.set_title(&quot;GPU Result&quot;)
ax2.imshow(field)
ax2.set_title(&quot;Python Result&quot;)
pyplot.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/diffusion_opencl_17_0.png" src="_images/diffusion_opencl_17_0.png" />
</div>
</div>
<p>That should already be a lot faster than our previous Python implementation, but we can do much better if we optimize our GPU kernel. And that is exactly what the rest of this tutorial is about!</p>
<p>Also, if you think the Python boilerplate code to call a GPU kernel was a bit messy, we’ve got good news for you! From now on, we’ll only use the Kernel Tuner to compile and benchmark GPU kernels, which we can do with much cleaner Python code.</p>
</section>
<section id="Auto-Tuning-with-the-Kernel-Tuner">
<h2>Auto-Tuning with the Kernel Tuner<a class="headerlink" href="#Auto-Tuning-with-the-Kernel-Tuner" title="Permalink to this heading">¶</a></h2>
<p>Remember that previously we’ve set the thread block dimensions to 16 by 16. But how do we actually know if that is the best performing setting? That is where auto-tuning comes into play. Basically, it is very difficult to provide an answer through performance modeling and as such, we’d rather use the Kernel Tuner to compile and benchmark all possible kernel configurations.</p>
<p>But before we continue, we’ll increase the problem size, because the GPU is very likely underutilized.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>nx = 4096
ny = 4096
field = get_initial_conditions(nx, ny)
kernel_string = get_kernel_string(nx, ny)
</pre></div>
</div>
</div>
<p>The above code block has generated new initial conditions and a new string that contains our OpenCL kernel using our new domain size.</p>
<p>To call the Kernel Tuner, we have to specify the tunable parameters, in our case <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code>. For this purpose, we’ll create an ordered dictionary to store the tunable parameters. The keys will be the name of the tunable parameter, and the corresponding value is the list of possible values for the parameter. For the purpose of this tutorial, we’ll use a small number of commonly used values for the thread block dimensions, but feel free to try more!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from collections import OrderedDict
tune_params = OrderedDict()
tune_params[&quot;block_size_x&quot;] = [16, 32, 48, 64, 128]
tune_params[&quot;block_size_y&quot;] = [2, 4, 8, 16, 32]
</pre></div>
</div>
</div>
<p>We also have to tell the Kernel Tuner about the argument list of our OpenCL kernel. Because the Kernel Tuner will be calling the OpenCL kernel and measure its execution time. For this purpose we create a list in Python, that corresponds with the argument list of the <code class="docutils literal notranslate"><span class="pre">diffuse_kernel</span></code> OpenCL function. This list will only be used as input to the kernel during tuning. The objects in the list should be Numpy arrays or scalars.</p>
<p>Because you can specify the arguments as Numpy arrays, the Kernel Tuner will take care of allocating GPU memory and copying the data to the GPU.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>args = [field, field]
</pre></div>
</div>
</div>
<p>We’re almost ready to call the Kernel Tuner, we just need to set how large the problem is we are currently working on by setting a <code class="docutils literal notranslate"><span class="pre">problem_size</span></code>. The Kernel Tuner knows about thread block dimensions, which it expects to be called <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code>, <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code>, and/or <code class="docutils literal notranslate"><span class="pre">block_size_z</span></code>. From these and the <code class="docutils literal notranslate"><span class="pre">problem_size</span></code>, the Kernel Tuner will compute the appropiate grid dimensions on the fly.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>problem_size = (nx, ny)
</pre></div>
</div>
</div>
<p>And that’s everything the Kernel Tuner needs to know to be able to start tuning our kernel. Let’s give it a try by executing the next code block!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from kernel_tuner import tune_kernel
result = tune_kernel(&quot;diffuse_kernel&quot;, kernel_string, problem_size, args, tune_params)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using: GeForce GTX TITAN X
block_size_x=16, block_size_y=2, time=1.1748096
block_size_x=16, block_size_y=4, time=0.7284544
block_size_x=16, block_size_y=8, time=0.7707904
block_size_x=16, block_size_y=16, time=0.8573184
block_size_x=16, block_size_y=32, time=0.8380288
block_size_x=32, block_size_y=2, time=0.686528
block_size_x=32, block_size_y=4, time=0.69648
block_size_x=32, block_size_y=8, time=0.7461632
block_size_x=32, block_size_y=16, time=0.818304
block_size_x=32, block_size_y=32, time=0.771072
block_size_x=48, block_size_y=2, time=0.7190464
block_size_x=48, block_size_y=4, time=0.7522432
block_size_x=48, block_size_y=8, time=0.7982208
block_size_x=48, block_size_y=16, time=0.9624512
block_size_x=64, block_size_y=2, time=0.7214464
block_size_x=64, block_size_y=4, time=0.7453312
block_size_x=64, block_size_y=8, time=0.8028416
block_size_x=64, block_size_y=16, time=0.8922624
block_size_x=128, block_size_y=2, time=0.747328
block_size_x=128, block_size_y=4, time=0.7860736
block_size_x=128, block_size_y=8, time=0.8637184
best performing configuration: block_size_x=32, block_size_y=2, time=0.686528
</pre></div></div>
</div>
<p>Note that the Kernel Tuner prints a lot of useful information. To ensure you’ll be able to tell what was measured in this run the Kernel Tuner always prints the GPU or OpenCL Device name that is being used, as well as the name of the kernel. After that every line contains the combination of parameters and the time that was measured during benchmarking. The time that is being printed is in milliseconds and is obtained by averaging the execution time of 7 runs of the kernel. Finally, as a matter
of convenience, the Kernel Tuner also prints the best performing combination of tunable parameters. However, later on in this tutorial we’ll explain how to analyze and store the tuning results using Python.</p>
<p>Looking at the results printed above, the difference in performance between the different kernel configurations may seem very little. However, on our hardware, the performance of this kernel already varies in the order of 10%. Which of course can build up to large differences in the execution time if the kernel is to be executed thousands of times. We can also see that the performance of the best configuration in this set is 5% better than our initially guessed thread block dimensions of 16 by
16.</p>
<p>In addtion, you may notice that not all possible combinations of values for <code class="docutils literal notranslate"><span class="pre">block_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">block_size_y</span></code> are among the results. For example, 128x32 is not among the results. This is because some configuration require more threads per thread block than allowed on our GPU. The Kernel Tuner checks the limitations of your GPU at runtime and automatically skips over configurations that use too many threads per block. It will also do this for kernels that cannot be compiled because they use
too much shared memory. And likewise for kernels that use too many registers to be launched at runtime. If you’d like to know about which configurations were skipped automatically you can pass the optional parameter <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> to <code class="docutils literal notranslate"><span class="pre">tune_kernel</span></code>.</p>
<p>However, knowing the best performing combination of tunable parameters becomes even more important when we start to further optimize our OpenCL kernel. In the next section, we’ll add a simple code optimization and show how this affects performance.</p>
</section>
<section id="Using-Shared-(local)-Memory">
<h2>Using Shared (local) Memory<a class="headerlink" href="#Using-Shared-(local)-Memory" title="Permalink to this heading">¶</a></h2>
<p>Shared (or local) memory, is a special type of the memory available in OpenCL. Shared memory can be used by threads within the same thread block to exchange and share values. It is in fact, one of the very few ways for threads to communicate on the GPU.</p>
<p>The idea is that we’ll try improve the performance of our kernel by using shared memory as a software controlled cache. There are already caches on the GPU, but most GPUs only cache accesses to global memory in L2. Shared memory is closer to the multiprocessors where the thread blocks are executed, comparable to an L1 cache.</p>
<p>However, because there are also hardware caches, the performance improvement from this step is expected to not be that great. The more fine-grained control that we get by using a software managed cache, rather than a hardware implemented cache, comes at the cost of some instruction overhead. In fact, performance is quite likely to degrade a little. However, this intermediate step is necessary for the next optimization step we have in mind.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>kernel_string = &quot;&quot;&quot;
#define nx %d
#define ny %d
#define dt 0.225f
__kernel void diffuse_kernel(global float *u_new, global float *u) {

    int tx = get_local_id(0);
    int ty = get_local_id(1);
    int bx = get_group_id(0) * block_size_x;
    int by = get_group_id(1) * block_size_y;

    __local float sh_u[block_size_y+2][block_size_x+2];

    #pragma unroll
    for (int i = ty; i&lt;block_size_y+2; i+=block_size_y) {
        #pragma unroll
        for (int j = tx; j&lt;block_size_x+2; j+=block_size_x) {
            int y = by+i-1;
            int x = bx+j-1;
            if (x&gt;=0 &amp;&amp; x&lt;nx &amp;&amp; y&gt;=0 &amp;&amp; y&lt;ny) {
                sh_u[i][j] = u[y*nx+x];
            }
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);  // __syncthreads();

    int x = bx+tx;
    int y = by+ty;
    if (x&gt;0 &amp;&amp; x&lt;nx-1 &amp;&amp; y&gt;0 &amp;&amp; y&lt;ny-1) {
        int i = ty+1;
        int j = tx+1;
        u_new[y*nx+x] = sh_u[i][j] + dt * (
            sh_u[i+1][j] + sh_u[i][j+1] -4.0f * sh_u[i][j] +
            sh_u[i][j-1] + sh_u[i-1][j] );
    }

}
&quot;&quot;&quot; % (nx, ny)
result = tune_kernel(&quot;diffuse_kernel&quot;, kernel_string, problem_size, args, tune_params)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using: GeForce GTX TITAN X
block_size_x=16, block_size_y=2, time=1.8449472
block_size_x=16, block_size_y=4, time=1.1912576
block_size_x=16, block_size_y=8, time=1.1035136
block_size_x=16, block_size_y=16, time=1.0927808
block_size_x=16, block_size_y=32, time=1.1140736
block_size_x=32, block_size_y=2, time=1.1790336
block_size_x=32, block_size_y=4, time=1.0808192
block_size_x=32, block_size_y=8, time=1.0809792
block_size_x=32, block_size_y=16, time=1.0836928
block_size_x=32, block_size_y=32, time=1.1545856
block_size_x=48, block_size_y=2, time=1.1249984
block_size_x=48, block_size_y=4, time=1.1264
block_size_x=48, block_size_y=8, time=1.1230336
block_size_x=48, block_size_y=16, time=1.4015104
block_size_x=64, block_size_y=2, time=1.0873216
block_size_x=64, block_size_y=4, time=1.0626496
block_size_x=64, block_size_y=8, time=1.0692224
block_size_x=64, block_size_y=16, time=1.140192
block_size_x=128, block_size_y=2, time=1.0801344
block_size_x=128, block_size_y=4, time=1.0688128
block_size_x=128, block_size_y=8, time=1.1428928
best performing configuration: block_size_x=64, block_size_y=4, time=1.0626496
</pre></div></div>
</div>
</section>
<section id="Tiling-GPU-Code">
<h2>Tiling GPU Code<a class="headerlink" href="#Tiling-GPU-Code" title="Permalink to this heading">¶</a></h2>
<p>One very useful code optimization is called tiling, sometimes also called thread-block-merge. You can look at it in this way, currently we have many thread blocks that together work on the entire domain. If we were to use only half of the number of thread blocks, every thread block would need to double the amount of work it performs to cover the entire domain. However, the threads may be able to reuse part of the data and computation that is required to process a single output element for every
element beyond the first.</p>
<p>This is a code optimization because effectively we are reducing the total number of instructions executed by all threads in all thread blocks. So in a way, were are condensing the total instruction stream while keeping the all the really necessary compute instructions. More importantly, we are increasing data reuse, where previously these values would have been reused from the cache or in the worst-case from GPU memory.</p>
<p>We can apply tiling in both the x and y-dimensions. This also introduces two new tunable parameters, namely the tiling factor in x and y, which we will call <code class="docutils literal notranslate"><span class="pre">tile_size_x</span></code> and <code class="docutils literal notranslate"><span class="pre">tile_size_y</span></code>. This is what the new kernel looks like:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>kernel_string = &quot;&quot;&quot;
#define nx %d
#define ny %d
#define dt 0.225f
__kernel void diffuse_kernel(global float *u_new, global float *u) {
    int tx = get_local_id(0);
    int ty = get_local_id(1);
    int bx = get_group_id(0) * block_size_x * tile_size_x;
    int by = get_group_id(1) * block_size_y * tile_size_y;

    /*
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int bx = blockIdx.x * block_size_x * tile_size_x;
    int by = blockIdx.y * block_size_y * tile_size_y;
    */
    __local float sh_u[block_size_y*tile_size_y+2][block_size_x*tile_size_x+2];

    #pragma unroll
    for (int i = ty; i&lt;block_size_y*tile_size_y+2; i+=block_size_y) {
        #pragma unroll
        for (int j = tx; j&lt;block_size_x*tile_size_x+2; j+=block_size_x) {
            int y = by+i-1;
            int x = bx+j-1;
            if (x&gt;=0 &amp;&amp; x&lt;nx &amp;&amp; y&gt;=0 &amp;&amp; y&lt;ny) {
                sh_u[i][j] = u[y*nx+x];
            }
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);  // __syncthreads();

    #pragma unroll
    for (int tj=0; tj&lt;tile_size_y; tj++) {
        int i = ty+tj*block_size_y+1;
        int y = by + ty + tj*block_size_y;
        #pragma unroll
        for (int ti=0; ti&lt;tile_size_x; ti++) {
            int j = tx+ti*block_size_x+1;
            int x = bx + tx + ti*block_size_x;
            if (x&gt;0 &amp;&amp; x&lt;nx-1 &amp;&amp; y&gt;0 &amp;&amp; y&lt;ny-1) {
                u_new[y*nx+x] = sh_u[i][j] + dt * (
                    sh_u[i+1][j] + sh_u[i][j+1] -4.0f * sh_u[i][j] +
                    sh_u[i][j-1] + sh_u[i-1][j] );
            }
        }

    }

}
&quot;&quot;&quot; % (nx, ny)
</pre></div>
</div>
</div>
<p>We can tune our tiled kernel by adding the two new tunable parameters to our dictionary <code class="docutils literal notranslate"><span class="pre">tune_params</span></code>.</p>
<p>We also need to somehow tell the Kernel Tuner to use fewer thread blocks to launch kernels with <code class="docutils literal notranslate"><span class="pre">tile_size_x</span></code> or <code class="docutils literal notranslate"><span class="pre">tile_size_y</span></code> larger than one. For this purpose the Kernel Tuner’s <code class="docutils literal notranslate"><span class="pre">tune_kernel</span></code> function supports two optional arguments, called grid_div_x and grid_div_y. These are the grid divisor lists, which are lists of strings containing all the tunable parameters that divide a certain grid dimension. So far, we have been using the default settings for these, in which case the Kernel
Tuner only uses the block_size_x and block_size_y tunable parameters to divide the problem_size.</p>
<p>Note that the Kernel Tuner will replace the values of the tunable parameters inside the strings and use the product of the parameters in the grid divisor list to compute the grid dimension rounded up. You can even use arithmetic operations, inside these strings as they will be evaluated. As such, we could have used <code class="docutils literal notranslate"><span class="pre">[&quot;block_size_x*tile_size_x&quot;]</span></code> to get the same result.</p>
<p>We are now ready to call the Kernel Tuner again and tune our tiled kernel. Let’s execute the following code block, note that it may take a while as the number of kernel configurations that the Kernel Tuner will try has just been increased with a factor of 9!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tune_params[&quot;tile_size_x&quot;] = [1,2,4]            #add tile_size_x to the tune_params
tune_params[&quot;tile_size_y&quot;] = [1,2,4]            #add tile_size_y to the tune_params
grid_div_x = [&quot;block_size_x&quot;, &quot;tile_size_x&quot;]    #tile_size_x impacts grid dimensions
grid_div_y = [&quot;block_size_y&quot;, &quot;tile_size_y&quot;]    #tile_size_y impacts grid dimensions
result = tune_kernel(&quot;diffuse_kernel&quot;, kernel_string, problem_size, args,
                     tune_params, grid_div_x=grid_div_x, grid_div_y=grid_div_y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Using: GeForce GTX TITAN X
block_size_x=16, block_size_y=2, tile_size_x=1, tile_size_y=1, time=1.8844544
block_size_x=16, block_size_y=2, tile_size_x=1, tile_size_y=2, time=1.3245952
block_size_x=16, block_size_y=2, tile_size_x=1, tile_size_y=4, time=1.0911808
block_size_x=16, block_size_y=2, tile_size_x=2, tile_size_y=1, time=1.3039616
block_size_x=16, block_size_y=2, tile_size_x=2, tile_size_y=2, time=1.0079296
block_size_x=16, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.84848
block_size_x=16, block_size_y=2, tile_size_x=4, tile_size_y=1, time=1.0708288
block_size_x=16, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.857728
block_size_x=16, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.7561792
block_size_x=16, block_size_y=4, tile_size_x=1, tile_size_y=1, time=1.231072
block_size_x=16, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.8774336
block_size_x=16, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.7087296
block_size_x=16, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.8772672
block_size_x=16, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.6911872
block_size_x=16, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.5715968
block_size_x=16, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.7584896
block_size_x=16, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.6292032
block_size_x=16, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.6498688
block_size_x=16, block_size_y=8, tile_size_x=1, tile_size_y=1, time=1.1145664
block_size_x=16, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.8252928
block_size_x=16, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.6757568
block_size_x=16, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.7881152
block_size_x=16, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.6237696
block_size_x=16, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.544224
block_size_x=16, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.6951168
block_size_x=16, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.5648128
block_size_x=16, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.6452736
block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=1, time=1.1065792
block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.8313792
block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.6905984
block_size_x=16, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.8302656
block_size_x=16, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.6367488
block_size_x=16, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.5478592
block_size_x=16, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.6660672
block_size_x=16, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.5719744
block_size_x=16, block_size_y=16, tile_size_x=4, tile_size_y=4, time=0.6551744
block_size_x=16, block_size_y=32, tile_size_x=1, tile_size_y=1, time=1.1384064
block_size_x=16, block_size_y=32, tile_size_x=1, tile_size_y=2, time=0.8531072
block_size_x=16, block_size_y=32, tile_size_x=1, tile_size_y=4, time=0.7078976
block_size_x=16, block_size_y=32, tile_size_x=2, tile_size_y=1, time=0.8516672
block_size_x=16, block_size_y=32, tile_size_x=2, tile_size_y=2, time=0.6677696
block_size_x=16, block_size_y=32, tile_size_x=2, tile_size_y=4, time=0.5685632
block_size_x=16, block_size_y=32, tile_size_x=4, tile_size_y=1, time=0.7074048
block_size_x=16, block_size_y=32, tile_size_x=4, tile_size_y=2, time=0.5753152
block_size_x=16, block_size_y=32, tile_size_x=4, tile_size_y=4, time=0.8228864
block_size_x=32, block_size_y=2, tile_size_x=1, tile_size_y=1, time=1.2124736
block_size_x=32, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.8633344
block_size_x=32, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.6921216
block_size_x=32, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.8896384
block_size_x=32, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.6659904
block_size_x=32, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.5582144
block_size_x=32, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.7522624
block_size_x=32, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.6081536
block_size_x=32, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.6664448
block_size_x=32, block_size_y=4, tile_size_x=1, tile_size_y=1, time=1.1095936
block_size_x=32, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.8063424
block_size_x=32, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.6717888
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.7982848
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.6263552
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.5289728
block_size_x=32, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.7008832
block_size_x=32, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.567456
block_size_x=32, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.5968704
block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=1, time=1.1018432
block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.8117248
block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.6724736
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.7728576
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.6038336
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.5172352
block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.6796352
block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.5470016
block_size_x=32, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.5968448
block_size_x=32, block_size_y=16, tile_size_x=1, tile_size_y=1, time=1.1107712
block_size_x=32, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.8237248
block_size_x=32, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.6810944
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.821952
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.620352
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.5230208
block_size_x=32, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.6415552
block_size_x=32, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.5476864
block_size_x=32, block_size_y=16, tile_size_x=4, tile_size_y=4, time=0.7168192
block_size_x=32, block_size_y=32, tile_size_x=1, tile_size_y=1, time=1.1942016
block_size_x=32, block_size_y=32, tile_size_x=1, tile_size_y=2, time=0.8626304
block_size_x=32, block_size_y=32, tile_size_x=1, tile_size_y=4, time=0.7099712
block_size_x=32, block_size_y=32, tile_size_x=2, tile_size_y=1, time=0.9123328
block_size_x=32, block_size_y=32, tile_size_x=2, tile_size_y=2, time=0.6608448
block_size_x=32, block_size_y=32, tile_size_x=2, tile_size_y=4, time=0.5631168
block_size_x=32, block_size_y=32, tile_size_x=4, tile_size_y=1, time=0.7113024
block_size_x=32, block_size_y=32, tile_size_x=4, tile_size_y=2, time=0.556576
block_size_x=48, block_size_y=2, tile_size_x=1, tile_size_y=1, time=1.1583104
block_size_x=48, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.8384832
block_size_x=48, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.67856
block_size_x=48, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.845856
block_size_x=48, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.6581248
block_size_x=48, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.54944
block_size_x=48, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.7520064
block_size_x=48, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.6076224
block_size_x=48, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.6842112
block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=1, time=1.1547072
block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.8422016
block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.6895552
block_size_x=48, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.8037312
block_size_x=48, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.6387072
block_size_x=48, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.5383296
block_size_x=48, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.7326656
block_size_x=48, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.5863488
block_size_x=48, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.6813376
block_size_x=48, block_size_y=8, tile_size_x=1, tile_size_y=1, time=1.1493952
block_size_x=48, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.8444928
block_size_x=48, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.6929216
block_size_x=48, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.832768
block_size_x=48, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.6389312
block_size_x=48, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.5412672
block_size_x=48, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.698336
block_size_x=48, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.5717568
block_size_x=48, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.676096
block_size_x=48, block_size_y=16, tile_size_x=1, tile_size_y=1, time=1.4303104
block_size_x=48, block_size_y=16, tile_size_x=1, tile_size_y=2, time=1.0341696
block_size_x=48, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.8365184
block_size_x=48, block_size_y=16, tile_size_x=2, tile_size_y=1, time=1.0398656
block_size_x=48, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.7786496
block_size_x=48, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.648928
block_size_x=48, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.8479232
block_size_x=48, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.6508544
block_size_x=64, block_size_y=2, tile_size_x=1, tile_size_y=1, time=1.1219392
block_size_x=64, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.7994048
block_size_x=64, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.6492288
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.8068416
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.6343168
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.5235328
block_size_x=64, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.7268928
block_size_x=64, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.5898432
block_size_x=64, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.6633536
block_size_x=64, block_size_y=4, tile_size_x=1, tile_size_y=1, time=1.0849664
block_size_x=64, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.7869632
block_size_x=64, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.6458624
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.7611968
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.613088
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.50912
block_size_x=64, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.6972928
block_size_x=64, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.5620608
block_size_x=64, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.601856
block_size_x=64, block_size_y=8, tile_size_x=1, tile_size_y=1, time=1.095232
block_size_x=64, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.7967488
block_size_x=64, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.6601472
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.7952896
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.6047296
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.5108224
block_size_x=64, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.6607744
block_size_x=64, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.5492416
block_size_x=64, block_size_y=8, tile_size_x=4, tile_size_y=4, time=0.7091136
block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=1, time=1.171552
block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=2, time=0.8473408
block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=4, time=0.6962112
block_size_x=64, block_size_y=16, tile_size_x=2, tile_size_y=1, time=0.8663936
block_size_x=64, block_size_y=16, tile_size_x=2, tile_size_y=2, time=0.6466816
block_size_x=64, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.5475584
block_size_x=64, block_size_y=16, tile_size_x=4, tile_size_y=1, time=0.6754048
block_size_x=64, block_size_y=16, tile_size_x=4, tile_size_y=2, time=0.5591744
block_size_x=128, block_size_y=2, tile_size_x=1, tile_size_y=1, time=1.108896
block_size_x=128, block_size_y=2, tile_size_x=1, tile_size_y=2, time=0.7907264
block_size_x=128, block_size_y=2, tile_size_x=1, tile_size_y=4, time=0.6459328
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=1, time=0.7965888
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=2, time=0.6250816
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.5188416
block_size_x=128, block_size_y=2, tile_size_x=4, tile_size_y=1, time=0.721408
block_size_x=128, block_size_y=2, tile_size_x=4, tile_size_y=2, time=0.5920832
block_size_x=128, block_size_y=2, tile_size_x=4, tile_size_y=4, time=0.7068608
block_size_x=128, block_size_y=4, tile_size_x=1, tile_size_y=1, time=1.0909248
block_size_x=128, block_size_y=4, tile_size_x=1, tile_size_y=2, time=0.7930752
block_size_x=128, block_size_y=4, tile_size_x=1, tile_size_y=4, time=0.6524544
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=1, time=0.7745216
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=2, time=0.6146176
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.5116928
block_size_x=128, block_size_y=4, tile_size_x=4, tile_size_y=1, time=0.6975872
block_size_x=128, block_size_y=4, tile_size_x=4, tile_size_y=2, time=0.5548416
block_size_x=128, block_size_y=4, tile_size_x=4, tile_size_y=4, time=0.7075136
block_size_x=128, block_size_y=8, tile_size_x=1, tile_size_y=1, time=1.174624
block_size_x=128, block_size_y=8, tile_size_x=1, tile_size_y=2, time=0.8384512
block_size_x=128, block_size_y=8, tile_size_x=1, tile_size_y=4, time=0.69104
block_size_x=128, block_size_y=8, tile_size_x=2, tile_size_y=1, time=0.8335488
block_size_x=128, block_size_y=8, tile_size_x=2, tile_size_y=2, time=0.6264192
block_size_x=128, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.5445248
block_size_x=128, block_size_y=8, tile_size_x=4, tile_size_y=1, time=0.6719104
block_size_x=128, block_size_y=8, tile_size_x=4, tile_size_y=2, time=0.5592064
best performing configuration: block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.50912
</pre></div></div>
</div>
<p>We can see that the number of kernel configurations tried by the Kernel Tuner is growing rather quickly. Also, the best performing configuration quite a bit faster than the best kernel before we started optimizing. On our GTX Titan X, the execution time went from 0.72 ms to 0.53 ms, a performance improvement of 26%!</p>
<p>Note that the thread block dimensions for this kernel configuration are also different. Without optimizations the best performing kernel used a thread block of 32x2, after we’ve added tiling the best performing kernel uses thread blocks of size 64x4, which is four times as many threads! Also the amount of work increased with tiling factors 2 in the x-direction and 4 in the y-direction, increasing the amount of work per thread block by a factor of 8. The difference in the area processed per
thread block between the naive and the tiled kernel is a factor 32.</p>
<p>However, there are actually several kernel configurations that come close. The following Python code prints all instances with an execution time within 5% of the best performing configuration.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>best_time = min(result[0], key=lambda x:x[&#39;time&#39;])[&#39;time&#39;]
for i in result[0]:
    if i[&quot;time&quot;] &lt; best_time*1.05:
        print(&quot;&quot;.join([k + &quot;=&quot; + str(v) + &quot;, &quot; for k,v in i.items()]))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
block_size_x=32, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.5289728,
block_size_x=32, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.5172352,
block_size_x=32, block_size_y=16, tile_size_x=2, tile_size_y=4, time=0.5230208,
block_size_x=64, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.5235328,
block_size_x=64, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.50912,
block_size_x=64, block_size_y=8, tile_size_x=2, tile_size_y=4, time=0.5108224,
block_size_x=128, block_size_y=2, tile_size_x=2, tile_size_y=4, time=0.5188416,
block_size_x=128, block_size_y=4, tile_size_x=2, tile_size_y=4, time=0.5116928,
</pre></div></div>
</div>
</section>
<section id="Storing-the-results">
<h2>Storing the results<a class="headerlink" href="#Storing-the-results" title="Permalink to this heading">¶</a></h2>
<p>While it’s nice that the Kernel Tuner prints the tuning results to stdout, it’s not that great if we’d have to parse what is printed to get the results. That is why the <code class="docutils literal notranslate"><span class="pre">tune_kernel()</span></code> returns a data structure that holds all the results. We’ve actually already used this data in the above bit of Python code.</p>
<p><code class="docutils literal notranslate"><span class="pre">tune_kernel</span></code> returns a list of dictionaries, where each benchmarked kernel is represented by a dictionary containing the tunable parameters for that particular kernel configuration and one more entry called ‘time’. The list of dictionaries format is very flexible and can easily be converted other formats that are easy to parse formats, like json or csv, for further analysis.</p>
<p>You can execute the following code block to store the tuning results to both a json and a csv file (if you have Pandas installed).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#store output as json
import json
with open(&quot;tutorial.json&quot;, &#39;w&#39;) as fp:
    json.dump(result[0], fp)

#store output as csv
from pandas import DataFrame
df = DataFrame(result[0])
df.to_csv(&quot;tutorial.csv&quot;)
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ben van Werkhoven.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>