<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tuning Host Code &mdash; Kernel Tuner 1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=359c27e9"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Using structs" href="structs.html" />
    <link rel="prev" title="Correctness Verification" href="correctness.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="contents.html" class="icon icon-home">
            Kernel Tuner
          </a>
              <div class="version">
                1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Kernel Tuner</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_multiplication.html">Matrix multiplication</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Kernel Tuner Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Features</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="backends.html">Backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="cache_files.html">Cache files</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Correctness Verification</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tuning Host Code</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tuning-the-number-of-streams">Tuning the number of streams</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="structs.html">Using structs</a></li>
<li class="toctree-l1"><a class="reference internal" href="templates.html">Templated kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Optimization strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics and Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="observers.html">Observers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user-api.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="vocabulary.html">Parameter Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="design.html">Design documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribution guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="contents.html">Kernel Tuner</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="contents.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Tuning Host Code</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/KernelTuner/kernel_tuner/blob/master/doc/source/hostcode.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tuning-host-code">
<h1>Tuning Host Code<a class="headerlink" href="#tuning-host-code" title="Permalink to this heading">¶</a></h1>
<p>With the Kernel Tuner it is also possible to tune the host code of your GPU programs, or even just any C function for that matter.
Tuning host code can be useful when it contains parameters that have impact on the performance of kernel on the GPU, such as the number of
streams to use when executing a kernel across multiple streams. Another example is when you want to include the data transfers between
host and device into your tuning setup, or tune for different methods of moving data between host and device.</p>
<dl class="simple">
<dt>There are few differences with tuning just a single CUDA or OpenCL kernel, to list them:</dt><dd><ul class="simple">
<li><p>You have to specify the lang=”C” option</p></li>
<li><p>The C function should return a <code class="docutils literal notranslate"><span class="pre">float</span></code></p></li>
<li><p>You have to do your own timing and error handling in C</p></li>
<li><p>Data is not automatically copied to and from device memory. To use an array in host memory, pass in a <code class="xref py py-mod docutils literal notranslate"><span class="pre">numpy</span></code> array. To use an array
in device memory, pass in a <code class="xref py py-mod docutils literal notranslate"><span class="pre">cupy</span></code> array.</p></li>
</ul>
</dd>
</dl>
<p>You have to specify the language as “C” because the Kernel Tuner will be calling a host function. This means that the Kernel
Tuner will have to interface with C and in fact uses a different backend. This also means you can use this way of tuning
without having PyCuda installed, because the C functions interface calls the CUDA compiler directly.</p>
<p>The C function should return a float, this is the convention used by the Kernel Tuner. The returned float is also the number
that you are tuning for. Meaning that this does not necessarily needs to be time, you could also optimize a program for
a different quality, as long as you can express that quality in a single floating-point value. When benchmarking an instance
of the parameter space the returned floats will be averaged for the multiple runs in the same way as with direct CUDA or OpenCL kernel tuning.</p>
<p>By itself the C language does not provide any very precise timing functions. If you are tuning the host code of a CUDA program you can use
CUDA Events to do the timing for you. However, if you are using plain C then you have to supply your own timing function.
In the <a class="reference external" href="https://github.com/kerneltuner/kernel_tuner/blob/master/examples/c/vector_add.py">C vector add example</a> we are using the <code class="docutils literal notranslate"><span class="pre">omp_get_wtime()</span></code> function from OpenMP to measure time on the CPU.</p>
<section id="tuning-the-number-of-streams">
<h2>Tuning the number of streams<a class="headerlink" href="#tuning-the-number-of-streams" title="Permalink to this heading">¶</a></h2>
<p>The following describes the example in <code class="docutils literal notranslate"><span class="pre">examples/cuda/convolution_streams.py</span></code>.
In this example, the same convolution kernel is used as with correctness checking and convolution application example.</p>
<p>What is different is that we also supply the host code, which you can find in <code class="docutils literal notranslate"><span class="pre">examples/cuda/convolution_streams.cu</span></code>. It is a bit
too long and complex to include here, but we will explain what it does. The idea behind the host code is that the kernel computation
is spread across a number of CUDA streams. In this way, it is possible to overlap the data transfers from host to device with kernel execution, and with
transfers from the device back to the host.</p>
<p>The way we split the computation across streams is by dividing the problem in the y-dimension into chunks. The data transferred by the first stream is slightly
larger to account for the overlapping border between the data needed by different streams. Before the kernel in stream <cite>n</cite> can start executing the data transfers
in streams <cite>n</cite> and <cite>n-1</cite> have to be finished. To ensure the latter, we use CUDA Events and in particular cudaStreamWaitEvent(), which halts stream <cite>n</cite> until the
transfer in stream <cite>n-1</cite> has finished.</p>
<p>The way you use the Kernel Tuner to tune this CUDA program is very similar to when you are tuning a CUDA kernel directly, as you can see below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;convolution_streams.cu&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="linenos"> 2</span>    <span class="n">kernel_string</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="n">problem_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span>
<span class="linenos"> 5</span><span class="n">size</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">problem_size</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">input_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">problem_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">16</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">problem_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">16</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="nb">input</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="linenos">10</span><span class="nb">filter</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">17</span><span class="o">*</span><span class="mi">17</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="nb">filter</span><span class="p">]</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="n">tune_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="linenos">14</span><span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;block_size_x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)]</span>
<span class="linenos">15</span><span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;block_size_y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
<span class="linenos">16</span>
<span class="linenos">17</span><span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;tile_size_x&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="linenos">18</span><span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;tile_size_y&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="n">tune_params</span><span class="p">[</span><span class="s2">&quot;num_streams&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="n">grid_div_x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;block_size_x&quot;</span><span class="p">,</span> <span class="s2">&quot;tile_size_x&quot;</span><span class="p">]</span>
<span class="linenos">23</span><span class="n">grid_div_y</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;block_size_y&quot;</span><span class="p">,</span> <span class="s2">&quot;tile_size_y&quot;</span><span class="p">,</span> <span class="s2">&quot;num_streams&quot;</span><span class="p">]</span>
<span class="linenos">24</span>
<span class="linenos">25</span><span class="n">kernel_tuner</span><span class="o">.</span><span class="n">tune_kernel</span><span class="p">(</span><span class="s2">&quot;convolution_streams&quot;</span><span class="p">,</span> <span class="n">kernel_string</span><span class="p">,</span>
<span class="linenos">26</span>    <span class="n">problem_size</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">tune_params</span><span class="p">,</span>
<span class="linenos">27</span>    <span class="n">grid_div_y</span><span class="o">=</span><span class="n">grid_div_y</span><span class="p">,</span> <span class="n">grid_div_x</span><span class="o">=</span><span class="n">grid_div_x</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>In fact, the only differences with the simple convolution example are:</dt><dd><ul class="simple">
<li><p>The source file also contains host code</p></li>
<li><p>“num_streams” is added to the tuning parameters</p></li>
<li><p>“num_streams” is added to the “grid_div_y” list</p></li>
<li><p>The kernel_name “convolution_streams” is a C function</p></li>
<li><p>lang=”C” is used to tell this is a C function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">filter</span></code> is not passed as a constant memory argument</p></li>
</ul>
</dd>
</dl>
<p>Most differences have been explained, but we clarify a few things below.</p>
<p>The function that we are tuning is a C function that launches the CUDA kernel by itself, yet we supply the grid_div_x and
grid_div_y lists. We are, however, not required to do so. The C function could just compute the grid dimensions in whatever way it sees fit. Using grid_div_y
and grid_div_x at this point is matter of choice. To support this convenience, the values grid_size_x and grid_size_y are inserted by the Kernel Tuner into the
compiled C code. This way, you don’t have to compute the grid size in C, you can just use the grid dimensions as computed by the Kernel Tuner.</p>
<p>The filter is not passed separately as a constant memory argument, because the CudaMemcpyToSymbol operation is now performed by the C host function. Also,
because the code is compiled differently, we have no direct reference to the compiled module that is uploaded to the device and therefore we can not perform this
operation directly from Python. If you are tuning host code, you have the option to perform all memory allocations, frees, and memcpy operations inside the C host code,
that’s the purpose of host code after all. That is also why you have to do the timing yourself in C, as you may not want to include the time spent on memory
allocations and other setup into your time measurements.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="correctness.html" class="btn btn-neutral float-left" title="Correctness Verification" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="structs.html" class="btn btn-neutral float-right" title="Using structs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016-2024, Ben van Werkhoven, Alessio Sclocco, Stijn Heldens, Floris-Jan Willemsen, Willem-Jan Palenstijn, Bram Veenboer and Richard Schoonhoven.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>