

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Detailed documentation &mdash; kernel_tuner 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Convolution Example" href="convolution.html" />
    <link rel="prev" title="Tutorial" href="tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> kernel_tuner
          

          
          </a>

          
            
            
              <div class="version">
                0.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial.html">Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Detailed documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix.html">Matrix Multiply Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="correctness.html">Kernel Correctness Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="hostcode.html">Tuning Host Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="internal.html">Internal module documentation</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">kernel_tuner</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Detailed documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/details.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="toctree-wrapper compound">
</div>
<div class="section" id="detailed-documentation">
<span id="details"></span><h1>Detailed documentation<a class="headerlink" href="#detailed-documentation" title="Permalink to this headline">¶</a></h1>
<p>This file provides detailed information of how the kernel tuner can be
called including all the optional arguments.</p>
<dl class="py function">
<dt class="sig sig-object py" id="kernel_tuner.tune_kernel">
<span class="sig-prename descclassname"><span class="pre">kernel_tuner.</span></span><span class="sig-name descname"><span class="pre">tune_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_string</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">problem_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arguments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tune_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_x</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restrictions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">answer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">atol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">platform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_threads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_noodles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.tune_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Tune a CUDA kernel given a set of tunable parameters</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_name</strong> (<em>string</em>) – The name of the kernel in the code.</p></li>
<li><p><strong>kernel_string</strong> (<em>string</em><em> or </em><em>list</em>) – <p>The CUDA, OpenCL, or C kernel code as a string.
It is also allowed for the string to be a filename of the file
containing the code.</p>
<p>To support combined host and device code tuning for runtime
compiled device code, a list of filenames can be passed instead.
The first file in the list should be the file that contains the
host code. The host code is allowed to include or read as a string
any of the files in the list beyond the first.</p>
</p></li>
<li><p><strong>problem_size</strong> (<em>tuple</em><em>(</em><em>int</em><em> or </em><em>string</em><em>, </em><em>int</em><em> or </em><em>string</em><em>)</em>) – <p>A tuple containing the size from which the grid
dimensions of the kernel will be computed. Do not divide by
the thread block sizes, if this is necessary use grid_div_x/y to
specify.</p>
<p>If you want you can use a string to specify a problem
size, within these strings you are allowed to write Python
arithmetic and use the tuning parameter names as variables.
The kernel tuner will replace instances of the tuning parameter
names with their current value while iterating over the search
space. See the reduction CUDA example for an example use of this
feature.</p>
</p></li>
<li><p><strong>arguments</strong> (<em>list</em>) – A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for scalars.</p></li>
<li><p><strong>tune_params</strong> (<em>dict</em><em>( </em><em>string :</em><em> [</em><em>...</em><em>] </em><em>)</em>) – <p>A dictionary containing the parameter names as keys,
and lists of possible parameter settings as values.
The kernel tuner will try to compile and benchmark all possible
combinations of all possible values for all tuning parameters.
This typically results in a rather large search space of all
possible kernel configurations.
For each kernel configuration, each tuning parameter is
replaced at compile-time with its current value.
Currently, the kernel tuner uses the convention that the following
list of tuning parameters are used as thread block dimensions:</p>
<blockquote>
<div><ul>
<li><p>”block_size_x”   thread block (work group) x-dimension</p></li>
<li><p>”block_size_y”   thread block (work group) y-dimension</p></li>
<li><p>”block_size_z”   thread block (work group) z-dimension</p></li>
</ul>
</div></blockquote>
<p>Options for changing these defaults may be added later. If you
don’t want the thread block dimensions to be compiled in, you
may use the built-in variables blockDim.xyz in CUDA or the
built-in function get_local_size() in OpenCL instead.</p>
</p></li>
<li><p><strong>grid_div_x</strong> (<em>list</em>) – A list of names of the parameters whose values divide
the grid dimensions in the x-direction. Arithmetic expressions can be
used if necessary inside the string containing a parameter name. For
example, in some cases you may want to divide the problem size in the
x-dimension with the number of warps rather than the number of threads
in a block, in such cases one could use [“block_size_x/32”]. Note that
the product of all grid divisor expressions is computed before dividing
the problem_size in that dimension. Also note that the divison is treated
as a float divison and resulting grid dimensions will be rounded up to
the nearest integer number.
If not supplied, [“block_size_x”] will be used by default, if you do not
want any grid x-dimension divisors pass an empty list.</p></li>
<li><p><strong>grid_div_y</strong> (<em>list</em>) – A list of names of the parameters whose values divide
the grid dimensions in the y-direction, None by default. See grid_div_x
for more details.</p></li>
<li><p><strong>restrictions</strong> (<em>list</em>) – A list of strings containing boolean expression that
limit the search space in that they must be satisfied by the kernel
configuration. These expressions must be true for the configuration
to be part of the search space. For example:
restrictions=[“block_size_x==block_size_y*tile_size_y”] limits the
search to configurations where the block_size_x equals the product
of block_size_y and tile_size_y.
The default is None.</p></li>
<li><p><strong>answer</strong> (<em>list</em>) – A list of arguments, similar to what you pass to arguments,
that contains the expected output of the kernel after it has executed
and contains None for each argument that is input-only. The expected
output of the kernel will then be used to verify the correctness of
each kernel in the parameter space before it will be benchmarked.</p></li>
<li><p><strong>atol</strong> (<em>float</em>) – The maximum allowed absolute difference between two elements
in the output and the reference answer, as passed to numpy.allclose().
Ignored if you have not passed a reference answer. Default value is
1e-6, that is 0.000001.</p></li>
<li><p><strong>verbose</strong> (<em>boolean</em>) – <p>Sets whether or not to report about configurations that
were skipped during the search. This could be due to several reasons:</p>
<blockquote>
<div><ul>
<li><p>kernel configuration fails one or more restrictions</p></li>
<li><p>too many threads per thread block</p></li>
<li><p>too much shared memory used by the kernel</p></li>
<li><p>too many resources requested for launch</p></li>
</ul>
</div></blockquote>
<p>verbose is set to False by default.</p>
</p></li>
<li><p><strong>lang</strong> (<em>string</em>) – Specifies the language used for GPU kernels. The kernel_tuner
automatically detects the language, but if it fails, you may specify
the language using this argument, currently supported: “CUDA”, “OpenCL”, or “C”</p></li>
<li><p><strong>device</strong> (<em>int</em>) – CUDA/OpenCL device to use, in case you have multiple
CUDA-capable GPUs or OpenCL devices you may use this to select one,
0 by default. Ignored if you are tuning host code by passing lang=”C”.</p></li>
<li><p><strong>platform</strong> – OpenCL platform to use, in case you have multiple
OpenCL platforms you may use this to select one,
0 by default. Ignored if not using OpenCL.</p></li>
<li><p><strong>cmem_args</strong> (<em>dict</em><em>(</em><em>string: numpy object</em><em>)</em>) – CUDA-specific feature for specifying constant memory
arguments to the kernel. In OpenCL these are handled as normal
kernel arguments, but in CUDA you can copy to a symbol. The way you
specify constant memory arguments is by passing a dictionary with
strings containing the constant memory symbol name together with numpy
objects in the same way as normal kernel arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dictionary of all executed kernel configurations and their
execution times.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict( string, float )</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="kernel_tuner.run_kernel">
<span class="sig-prename descclassname"><span class="pre">kernel_tuner.</span></span><span class="sig-name descname"><span class="pre">run_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_string</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">problem_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arguments</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_x</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_div_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">platform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmem_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#kernel_tuner.run_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compile and run a single kernel</p>
<p>Compiles and runs a single kernel once, given a specific instance of the kernels tuning parameters.
This function was added to the kernel tuner mostly for verifying kernel correctness.
On purpose, it is called much in the same way as <cite>tune_kernel()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_name</strong> (<em>string</em>) – The name of the kernel in the code</p></li>
<li><p><strong>kernel_string</strong> (<em>string</em>) – The CUDA or OpenCL kernel code as a string</p></li>
<li><p><strong>problem_size</strong> (<em>tuple</em><em>(</em><em>int</em><em>, </em><em>int</em><em>)</em>) – A tuple containing the size from which the grid
dimensions of the kernel will be computed. Do not divide by
the thread block sizes, if this is necessary use grid_div_x/y to
specify.</p></li>
<li><p><strong>arguments</strong> (<em>list</em>) – A list of kernel arguments, use numpy arrays for
arrays, use numpy.int32 or numpy.float32 for singulars</p></li>
<li><p><strong>params</strong> (<em>dict</em><em>( </em><em>string: int</em><em> )</em>) – A dictionary containing the tuning parameter names as keys
and a single value per tuning parameter as values.</p></li>
<li><p><strong>grid_div_x</strong> (<em>list</em>) – See tune_kernel()</p></li>
<li><p><strong>grid_div_y</strong> (<em>list</em>) – See tune_kernel()</p></li>
<li><p><strong>lang</strong> (<em>string</em>) – Language of the kernel, supply “CUDA”, “OpenCL”, or “C” if not detected automatically.</p></li>
<li><p><strong>device</strong> (<em>int</em>) – CUDA/OpenCL device to use, 0 by default.</p></li>
<li><p><strong>platform</strong> – OpenCL platform to use, in case you have multiple
OpenCL platforms you may use this to select one,
0 by default. Ignored if not using OpenCL.</p></li>
<li><p><strong>cmem_args</strong> (<em>dict</em><em>(</em><em>string</em><em>, </em><em>...</em><em>)</em>) – CUDA-specific feature for specifying constant memory
arguments to the kernel. See tune_kernel() for details.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of numpy arrays, similar to the arguments passed to this
function, containing the output after kernel execution.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="convolution.html" class="btn btn-neutral float-right" title="Convolution Example" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="tutorial.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2016, Ben van Werkhoven.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>